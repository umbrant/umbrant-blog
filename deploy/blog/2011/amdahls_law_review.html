<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">

	<head>
		<title>
			
			Paper review: Amdahl&#39;s Law in the Multicore Era : umbrant
			
		</title>

		<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
		<link rel="icon" type="image/ico" href="/favicon.ico"/>

		
		<link rel="alternate" type="application/atom+xml" href="http://feeds.feedburner.com/UmbrantBlog" title="umbrant"/>
		

		
		<link href="/media/css/style.css" rel="stylesheet" type="text/css" media="screen" />
		<link href="/media/css/pygments.css" rel="stylesheet" type="text/css" media="screen" />
		<link href='http://fonts.googleapis.com/css?family=Quattrocento|Droid+Sans|Droid+Sans+Mono' rel='stylesheet' type='text/css'/>
		
		
		

		
		<script type="text/javascript" src="http://ajax.googleapis.com/ajax/libs/jquery/1.5.1/jquery.min.js"></script>
		<script type="text/javascript" src="/media/js/umbrant.js"></script>
		
		<script type="text/javascript">

  			var _gaq = _gaq || [];
  			_gaq.push(['_setAccount', 'UA-4601421-1']);
  			_gaq.push(['_trackPageview']);

  			(function() {
    		 var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    		 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    		 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  			 })();

		 </script>
		 
		 
	 </head>

	 <body>
		 <div id="wrapper">
			<div id="page">
				<div id="page-bgtop">
					<div id="page-bgbtm">
						<div id="content">

							

	


	

		<div class="post single">
			<h2 class="title"><a href="">Paper review: Amdahl&#39;s Law in the Multicore Era</a></h2>
			
			<p class="meta"><span class="date">September 11, 2011</span></p>
			
			<div class="entry">
			

			<!-- Hyde::Article::Begin -->

<!-- Hyde::Excerpt::Begin -->

<p>This is a paper review of "Amdahl's Law in the Multicore Era", by Hill and&nbsp;Marty.</p>
<!-- Hyde::Excerpt::End -->

<h3>Main&nbsp;ideas</h3>
<p>There are two important equations in this paper that lay the foundation for the rest of the paper: Amdahl's law, and the processor performance&nbsp;law.</p>
<pre><code>speedup = 1 / ((1-f + f/s)
perf(r) = sqrt(r)
</code></pre>
<p>These two equations have some deep implications. Starting with the latter, there are diminishing returns from investing more chip resources to single core performance. It's the reason why per-core performance has basically peaked in recent years, and everything is moving toward multicore. Adding more cache and beefing up adders can only do so much; since clock rates have peaked because of power limitations, we're basically stuck with the per-core performance we've&nbsp;got.</p>
<p>Moving on to Amdahl's law, <code>f</code> is the fraction of a program that is parallelizable, and <code>s</code> is the speedup of the parallelizable part. Interpreting this, we see there are essentially three ways of making your program run&nbsp;faster:</p>
<ol>
<li>Run the serial part faster (<code>1-f</code> component)</li>
<li>Make <code>s</code> bigger (increase the granularity of&nbsp;parallelism)</li>
<li>Make <code>f</code> bigger (parallelize serial parts of the&nbsp;code)</li>
</ol>
<p>Combining both equations, what are the limitations we&nbsp;see?</p>
<ul>
<li>Running the serial part faster gets expensive quickly in terms of chip&nbsp;resources.</li>
<li>Increasing the granularity of parallelism is a great approach, but only works really well for data-parallel tasks like serving webpages. Going beyond this natural parallelism into true fine-grained parallelism is expensive and&nbsp;limited.</li>
<li>Parallelizing serial parts of the code is also a great approach, but similarly limited. Some serial code simply can't be parallelized, and there's the same diminishing returns effect in terms of programmer&nbsp;time.</li>
</ul>
<p>The article goes on to talk about different ways of allocating chip resources in light of these two laws, covering the symmetric, asymmetric, and dynamic multicore chips. Basically, favoring bigger cores makes the serial part run faster, but reduces the number of ways you can split the parallel part, with smaller cores having the opposite effect. 
The net result of the paper is that it all depends on your workload. Asymmetric and dynamic multicore offer better performance characteristics than symmetric since they can better handle both the serial and parallel parts of a program&nbsp;well.</p>
<p>Applying this to cloud computing, we see that there are a lot of parallels between multicore and cloud computing. Instead of getting speedup by parallelizing at the level of instructions in a program, web services are typically scaled through request-level parallelism: distributing requests among a cluster of machines. In this case, the <code>1-f</code> factor in Amdahl's law can be effectively zero, since requests can be handled independently by each&nbsp;machine.</p>
<p>The ideas of asymmetric cores is also already seen in request-level parallelism since bigger tasks can be allocated to bigger&nbsp;machines.</p>
<h3>Future&nbsp;trends</h3>
<p>Servers processors are going to be manycore in the future, whether we want it or not. Peak per-core performance is probably not going to increase, and I don't hold out much hope for the effectiveness of dynamic multicore, but I think there's a strong argument for asymmetric multicore since there are demonstrated benefits over symmetric. Big cores will be used for serial computation, with data-parallel parts off-loaded to small cores as possible. This is also better for the coming scarcity of memory bandwidth; having big cores that can use it more effectively makes&nbsp;sense.</p>
<p>This also lends flexibility into request handling, since high-priority tasks can get the big core while latency-tolerant operations can run on a small core. The big problem here is going to be performance isolation for shared resources like mem, disk, and network; with more threads, there's more contention, and potentially more variability in performance (which is the killer). Scheduling also becomes more difficult, since "slots" on the same machine are now unequal in&nbsp;size.</p>
<p>For batch processing (like Hadoop), throughput matters more than latency. Here, SMP is the name of the game: we want to optimize <code>perf(r)</code> and have a bunch of wimpy cores. I still worry about I/O demands on durable storage, since HDD throughput doesn't seem to be increasing at the same rate that cores are being&nbsp;added.</p>
<p>Software stack&nbsp;predictions:</p>
<ul>
<li>We're going to see a focus on better asymmetric-aware, load-based cluster scheduling algorithms. This needs to balance out I/O load&nbsp;too.</li>
<li>We're going to see moderate fine-grained parallelism added to existing request-level parallelism, since it's the real only way of reducing&nbsp;latency. </li>
<li>As a corollary, I think there's going to be a big focus on performance isolation even at the cost of throughput, since variation in latency is the real&nbsp;killer.</li>
<li>MapReduce as a programming model and framework probably will not go away, but we're going to see mappers making better use of multi-core. Whether this is going to be MapReduce-in-MapReduce or OpenMP-in-MapReduce or Pthreads-in-MapReduce is unclear, but it makes sense to further break down an already data-parallel task into core-sized&nbsp;chunks.</li>
</ul>
<!-- Hyde::Article::End -->

			
			</div>
		</div>


	
<div id="disqus_thread"></div>
<script type="text/javascript">
    /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
    var disqus_shortname = 'umbrant'; // required: replace example with your forum shortname

    // The following are highly recommended additional parameters. Remove the slashes in front to use.
    var disqus_identifier = '/blog/2011/amdahls_law_review.html';
    var disqus_url = 'http://www.umbrant.com/blog/2011/amdahls_law_review.html';
    //var disqus_developer = 1;

    /* * * DON'T EDIT BELOW THIS LINE * * */
    (function() {
        var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
        dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
        (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">blog comments powered by <span class="logo-disqus">Disqus</span></a>




						</div>
						<!-- end #content -->

						<div id="sidebar">
	<div id="logo">
		<h1><a href="/">umbrant</a></h1>
	</div>
	<div>
		<p>My name is Andrew Wang. I'm a CS PhD student at UC Berkeley working in distributed systems.</p>
	</div>
	<div id="menu">
		<ul>
			<li><a href="/">Home</a></li>
			<li><a href="/about.html">About</a></li>
			<li><a href="/research.html">Research</a></li>
			<li><a href="/contact.html">Contact</a></li>
		</ul>
	</div>
	<ul>
		<li>
		<h2>Recent Posts</h2>
		<ul>
			
				
			
				

					
					
						
						<li><a href="/blog/2011/datacenter_needs_an_operating_system_review.html">Paper review: The Datacenter Needs an Operating System</a></li>
						
					
						
						<li><a href="/blog/2011/performance_modeling_and_analysis_of_flash_review.html">Paper review: Performance Modeling and Analysis of Flash-based Storage Devices</a></li>
						
					
						
						<li><a href="/blog/2011/data-level_parallelism_review.html">Paper review: Data-Level Parallelism in Vector, SIMD, and GPU Architectures</a></li>
						
					
						
						<li><a href="/blog/2011/amdahls_law_review.html">Paper review: Amdahl&#39;s Law in the Multicore Era</a></li>
						
					
						
						<li><a href="/blog/2011/datacenter_as_a_computer_3_4_7_review.html">Paper review: The Datacenter as a Computer Ch. 3, 4, 7</a></li>
						
					
						
						<li><a href="/blog/2011/warehouse_scale_computing_summary.html">Paper review: Warehouse-Scale Computing: Entering the Teenage Decade</a></li>
						
					
				
			
				
			
				
			
				
			
		</ul>
		</li>
		<li>
		<h2>Archive</h2>
		<ul>
			
			
			
			
				
				
				
				
				
				
				<li><a href="/blog/2011/2011.html">2011</a></li>
				
				
				
				
			
			
			
			
			
			
			
			
		</ul>
		</li>
	</ul>
</div>

						<!-- end #sidebar -->

									<div id="footer">
				<p>Copyright &copy; Andrew Wang, umbrant.com</p>
				<p>Licensed under <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/3.0/">CC BY-NC-SA 3.0</a></p>
				<br/>
				<p><a href="http://feeds.feedburner.com/UmbrantBlog"><img src="/media/images/feed-icon-14x14.png" alt="feed"/>&#160; Subscribe to my feed</a></p>
			</div>

						<!-- end #footer -->

					</div>
				</div>
			</div>
			<!-- end #page -->
		</div>
	 </body>
 </html>

 