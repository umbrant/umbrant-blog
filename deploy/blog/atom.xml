<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>umbrant blog</title><link href="http://www.umbrant.com/blog/atom.xml" rel="self"/><link href="http://www.umbrant.com"/><updated>2011-10-09T19:23:56Z</updated><id>http://www.umbrant.com</id><entry><title>Paper review: Hive and Pig</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/hive_pig_paper_review.html"/><updated>2011-10-09T17:44:00Z</updated><published>2011-10-09T17:44:00Z</published><id>http://www.umbrant.com/blog/2011/hive_pig_paper_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;Hive: Data Warehousing &lt;span class=&quot;amp&quot;&gt;&amp;amp;&lt;/span&gt; Analytics on Hadoop&amp;#8221; by the Facebook Data Team (a &lt;a href=&quot;http://www.slideshare.net/zshao/hive-data-warehousing-analytics-on-hadoop-presentation&quot;&gt;set of slides&lt;/a&gt;), and &amp;#8220;Pig Latin&amp;#8221; A Not-So-Foreign Language for Data Processing&amp;#8221; by Olston et al. The Hive slide deck I believe is from 2009, and Pig was published at &lt;span class=&quot;caps&quot;&gt;SIGMOD&lt;/span&gt; in 2008. I supplemented this with the Hive paper published at &lt;span class=&quot;caps&quot;&gt;VLDB&lt;/span&gt; in&amp;nbsp;2009.&lt;/p&gt;
&lt;p&gt;These are Facebook and Yahoo&amp;#8217;s approaches to higher-level languages that compile down to MapReduce on Hadoop. Measured by the percentage of Hive and Pig jobs on their production clusters, they have both been extremely successful. Hive takes a traditional &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt;/database-like approach, while Pig looks more imperative. At face value they seem quite different, but there are actually a bunch of underlying&amp;nbsp;similarities.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Hive main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;Hive is effectively a traditional database that just uses &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt; and MapReduce for data storage and query execution. Tables are serialized and deserialized to files in &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt;, and can be partitioned across and within fields. The query language, HiveQL, looks exactly like &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt; minus some of the more complicated operators because of engineering effort and the limitations of MapReduce. UDFs are also supported, meaning that normal MapReduce code can be slid right in. HiveQL is compiled down into a &lt;span class=&quot;caps&quot;&gt;MR&lt;/span&gt; query plan which can consist of multiple &lt;span class=&quot;caps&quot;&gt;MR&lt;/span&gt; jobs. The logical plan is optimized by a rule-based optimizer (future work being an adaptive cost-based&amp;nbsp;optimizer).&lt;/p&gt;
&lt;p&gt;Queries can be fed to the server via a Thrift server, which enables Hive usage from a variety of different programming languages. A small note is that table metadata is stored outside of &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt;, in a normal database. This is simply because the amount of data is small, and the access pattern is pretty random, making &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt;&amp;nbsp;ill-suited.&lt;/p&gt;
&lt;h3&gt;Pig main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;Pig is designed explicitly for ad-hoc data analysis by programmers. The query language looks like Python with operators pulled from &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt;, and instead of tables, users are given more programmer-friendly data structures like maps and lists. UDFs are also first-class citizens in Pig, and can have arbitrary inputs and outputs (non-atomic&amp;nbsp;values).&lt;/p&gt;
&lt;p&gt;All this means that the query language and data format are more flexible. Hive needs to the classic &lt;span class=&quot;caps&quot;&gt;ETL&lt;/span&gt; (extract-transform-load) to get data into tables before it can query it. Pig, you just pass it a file and a function explaining how to interpret it. This likely comes at a performance cost, but using the standard deserializers and a schema would ameliorate this. Pig also allows for more explicit control over the query plan, since each stage in the execution &lt;span class=&quot;caps&quot;&gt;DAG&lt;/span&gt; is as&amp;nbsp;programmed.&lt;/p&gt;
&lt;p&gt;As in Hive, Pig does not provide some operators because of the limitations of MapReduce. Also as in Hive, statements using the &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt;-like operators can be optimized, and multiple MapReduce jobs are chained together for you&amp;nbsp;automatically.&lt;/p&gt;
&lt;p&gt;One thing I really like about Pig is the focus on debugging. Trying to reason about a page of &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt; is really difficult, and it&amp;#8217;s much easier to reason about Pig&amp;#8217;s series of steps. It looks extremely similar to how I do ad-hoc text parsing in Python: gradually applying operators to collections of strings until I get the result I want. Pig also provides an &amp;#8220;example execution table&amp;#8221; that shows what the Pig program does on a small amount of data, which is much quicker than running the actual MapReduce&amp;nbsp;jobs.&lt;/p&gt;
&lt;h3&gt;Analysis&lt;/h3&gt;
&lt;p&gt;It&amp;#8217;s handy that both Hive and Pig automatically string together &lt;span class=&quot;caps&quot;&gt;MR&lt;/span&gt; jobs as part of one program, but you still pay the serialization overhead of writing things into intermediate files between jobs. This is something that isn&amp;#8217;t true with a more general execution framework like Dryad. The move towards more declarative languages, as I&amp;#8217;ve said previously, isn&amp;#8217;t surprising at all, since actually programming a MapReduce job is way more work than using something more high-level and declarative. For ad-hoc queries, it&amp;#8217;s way better to optimize for programmer productivity than try to squeeze out that last 20% of performance from writing it in raw&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;MR&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Hive has been extremely popular at Facebook, and I think the same is true of Pig at Yahoo. I think the future is going to be improving the underlying Hadoop execution engine to better support ad-hoc queries by keeping intermediate files in memory, and improving the number of operators and optimizers for both&amp;nbsp;languages.&lt;/p&gt;

   </content></entry><entry><title>Paper review: MapReduce and Dryad</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/dryad_mapreduce_review.html"/><updated>2011-09-30T11:58:00Z</updated><published>2011-09-30T11:58:00Z</published><id>http://www.umbrant.com/blog/2011/dryad_mapreduce_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is another combined paper review, because the ideas are again pretty similar, and it&amp;#8217;s a useful compare/contrast. The first is the famous MapReduce paper from Google, and the second is Microsoft&amp;#8217;s response,&amp;nbsp;Dryad.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;MapReduce: Simplified Data Processing on Large Clusters&amp;#8221;, Dean and Ghemawat. Published at &lt;span class=&quot;caps&quot;&gt;OSDI&lt;/span&gt;&amp;nbsp;2004.&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks&amp;#8221;, Isard et al. Published at Eurosys&amp;nbsp;2007.&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main ideas from&amp;nbsp;MapReduce&lt;/h3&gt;
&lt;p&gt;MapReduce is a parallel data processing framework designed initially for a very specific task: scanning large amounts of textual data to create a web search index. It was essentially codesigned with &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; for this purpose. As a result, it boils down computation into just two phases: &lt;em&gt;map&lt;/em&gt;, followed by &lt;em&gt;reduce&lt;/em&gt;. Programmers have to write just two functions, one for each phase. Then, these two functions are run in massively parallel fashion: mappers run the map function, and the output from mappers is then fed to reducers, which run the reduce function. Mappers are scheduled for data-locality, moving computation to where data is stored to minimize network communication. The map phase essentially does some data-parallel operation, while the reduce phase aggregates results from the map phase to produce the final&amp;nbsp;output.&lt;/p&gt;
&lt;p&gt;The cool parts of this paper are twofold: first, that such a simple, limited programming model can accommodate such a wide variety of tasks, and that almost all of the complexity of running code on thousands of machines can be abstracted away from the&amp;nbsp;programmer.&lt;/p&gt;
&lt;p&gt;Regarding the programming model, it&amp;#8217;s something that can be taught in a matter of days. Map and reduce are familiar from functional programming languages, and really small amounts of code can do very powerful things. It is quite limited (only works for data-parallel operations), but when dealing with big data, your operations basically have to be data-parallel to complete in any reasonable time. Google used MapReduce to do a wide variety of tasks, so the proof of utility is in the&amp;nbsp;pudding.&lt;/p&gt;
&lt;p&gt;The distributed, fault tolerant framework is what really drew me in. A single master manages all the workers (which potentially means a single point of failure), but this is way less likely than a worker failure. Failure of workers is handled transparently, by restarting the worker. This is possible because the output from the map stage is durably written to disk storage, then read by the reducers. Mapper input, of course, is durably stored as well, so they can also be easily restarted. Another feature I liked is chopping off the latency tail caused by &amp;#8220;straggler&amp;#8221; workers by starting duplicate tasks towards the end of the&amp;nbsp;job.&lt;/p&gt;
&lt;p&gt;In summation, MapReduce is both a powerful and simple framework that saw a lot of use at Google for a variety of&amp;nbsp;tasks.&lt;/p&gt;
&lt;h3&gt;Main ideas from&amp;nbsp;Dryad&lt;/h3&gt;
&lt;p&gt;Dryad is what some people see as &amp;#8220;MapReduce done right&amp;#8221;, but this is a contentious claim. It&amp;#8217;s a more general framework in two important ways. First, it allows for more general styles of computation, meaning more than just two phases, and more than just map and reduce elements in the graph. Second, it allows communication between stages to happen over more than just files stored in the &lt;span class=&quot;caps&quot;&gt;DFS&lt;/span&gt;: Dryad allows for sockets, shared memory, and pipes to be used as channels between elements. It ultimately ends up looking like a &lt;span class=&quot;caps&quot;&gt;DAG&lt;/span&gt; of user-defined elements. Data flows between elements over a choice of channels, and the elements are all user-defined. This has a number of benefits: more efficient communication, the ability to chain together multiple stages, and express more complicated&amp;nbsp;computation.&lt;/p&gt;
&lt;p&gt;This leads to a number of complications. While it does subsume the MapReduce paradigm, with generality comes complexity. The programming model is nowhere near as simple (the authors cite &amp;#8220;a couple weeks&amp;#8221; to get started), and to me, it feels like doing the work of a database: designing all the elements and communication in a physical query plan, and optimizing it. They even do a direct comparison against &lt;span class=&quot;caps&quot;&gt;SQL&lt;/span&gt; Server in the paper, in fact showing that they have similar query plans but Dryad comes out a little bit faster. Doing this really isn&amp;#8217;t simple at all, and the example queries they show do nothing to deny it. I consider dataflow programming (what this is, essentially) to be difficult to reason about for most&amp;nbsp;programmers.&lt;/p&gt;
&lt;p&gt;Dryad also incorporates the same fault-tolerance as MapReduce, and is able to restart failed tasks correctly. It also has this idea of &amp;#8220;dynamic runtime optimization&amp;#8221; which sounds very &lt;span class=&quot;caps&quot;&gt;DB&lt;/span&gt;, and is hard for MapReduce to do since it&amp;#8217;s the equivalent of UDFs in a&amp;nbsp;database.&lt;/p&gt;
&lt;h3&gt;Comparison and&amp;nbsp;evaluation&lt;/h3&gt;
&lt;p&gt;My impression is that people were pretty unhappy with Dryad when it came out. It&amp;#8217;s not nearly as elegant as MapReduce, there aren&amp;#8217;t any cool operational insights, and feels very &amp;#8220;me too&amp;#8221;. However, as stated in the Dryad paper, programmers aren&amp;#8217;t really meant to interface with Dryad directly, and are instead supposed to use things like DryadLINQ (which turns declarative &lt;span class=&quot;caps&quot;&gt;LINQ&lt;/span&gt; queries into Dryad execution graphs, exactly how everyone wanted). This is true for MapReduce too, since FlumeJava has seen heavy use at Google, and Hive and Pig dominate Hadoop workloads at Facebook. As nice and &amp;#8220;simple&amp;#8221; MapReduce is compared to Dryad, no one is directly programming on either these days, instead doing the &lt;span class=&quot;caps&quot;&gt;DB&lt;/span&gt;-like thing and using declarative query&amp;nbsp;languages.&lt;/p&gt;
&lt;p&gt;Dryad also did correctly identify all the flaws with MapReduce, flaws that have to be papered over and hacked around to get the same kind of performance and generality. Hadoop is going to have to become more memory aware to eke out additional performance, and there are &amp;#8220;workflow management&amp;#8221; tools that allow chaining of multiple MapReduce jobs to effectively achieve multi-stage workflows. As long as the user never has to worry about the details, declarative execution engines built on top of Dryad rather than Hadoop have an&amp;nbsp;advantage.&lt;/p&gt;
&lt;p&gt;In terms of future relevance, I think that the basic idea of hiding faults and communication from the programmer is totally the right idea. It&amp;#8217;s way easier to write programs within a Dryad or MapReduce framework than something like &lt;span class=&quot;caps&quot;&gt;MPI&lt;/span&gt;, which didn&amp;#8217;t hide anything. The &lt;span class=&quot;caps&quot;&gt;DB&lt;/span&gt; community had it right though in calling for declarative query languages, and Hadoop and MapReduce these days are essentially being used as distributed query execution engines. I think we&amp;#8217;re going to see a wider variety of query languages in the future though, since there&amp;#8217;s a tradeoff between generality and simplicity. I doubt Hive and FlumeJava are the final word. There&amp;#8217;s also room for other types of query execution engines; Pregel&amp;#8217;s &lt;span class=&quot;caps&quot;&gt;BSP&lt;/span&gt; is an&amp;nbsp;example.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Megastore</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/megastore_review.html"/><updated>2011-09-24T19:35:00Z</updated><published>2011-09-24T19:35:00Z</published><id>http://www.umbrant.com/blog/2011/megastore_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;Megastore: Providing Scalable, Highly Available Storage for Interactive Services&amp;#8221; by Baker et al. This was published at &lt;span class=&quot;caps&quot;&gt;CIDR&lt;/span&gt; in 2011. The basic idea is providing &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt; semantics across geographically-distant datacenters with highly partitioned datasets and an efficient Paxos replication&amp;nbsp;scheme.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;The basic premise of Megastore is that some applications require strong &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt; semantics, while also desiring the fault-tolerance that comes with cross-datacenter replication. They claim that existing solutions (like a heavily sharded MySQL database) do not fill this niche because they are hard to manage and scale, driving a need for Megastore. Megastore does this by asking application developers to partition their data into &lt;em&gt;entity groups&lt;/em&gt;, where each group represents a relatively small amount of data: the profile for one user, or a single blog account. Operations within the group get full &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt; semantics; cross group operations have to build their own consistency model, perhaps two-phase commit, or something looser. Megastore also allows applications to do less-consistent reads for lower&amp;nbsp;latency.&lt;/p&gt;
&lt;p&gt;The data model and query language for Megastore also differs from traditional RDBMSs. The data model isn&amp;#8217;t relational since it&amp;#8217;s built on top of Bigtable (which in turn, is on top of &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt;), but is still strongly-typed and consists of properties within tables. The query language is more limited; being based on Bigtable means that there isn&amp;#8217;t support for joins. This is fixed either by denormalizing the data, or doing it in application code. There seem to be a lot of tricks for creating indexes and doing data placement&amp;nbsp;efficiently.&lt;/p&gt;
&lt;p&gt;Log replication is done by using Paxos to resolve each log entry before applying it. Multiple writers race to get a single leader to accept their write; failed attempts have to be retried. Performance wise, they still have to do an inter-datacenter roundtrip even in the best case of a stable leader and being able to piggyback accepts and prepares. This means that they&amp;#8217;re never going to do better than a few writes per second; they quote a figure of 100-400ms latency per write. This is okay as long as the entity groups are small and the application write rates are thus low. Reads can be done without a roundtrip by having a special coordinator in each datacenter which tracks when replicas become out of&amp;nbsp;date.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;relevance&lt;/h3&gt;
&lt;p&gt;The biggest thing that stuck with me when reading this paper was that as a developer, this sounds really painful to use. Partitioning data that finely is painful, and you have to build inter-group consistency yourself. This indicates to me that schema changes might be common, but that&amp;#8217;s really painful since data is denormalized and there&amp;#8217;s all this schema-specific app-level code built on top to do joins and consistency. The claim that there is &amp;#8220;predictable performance&amp;#8221; from a lack of joins seems unsubstantiated. Building on top of Bigtable which is on top of Megastore means that it&amp;#8217;s very hard for developers to reason about what is actually going on under the hood. Furthermore, developers have to program around the super slow write rate. Hiding a slow Megastore write behind an asynchronous Javascript call sort of defeats the purpose of having&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Compared to other Google papers like &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; and MapReduce, Megastore just seems way too complicated. It doesn&amp;#8217;t convince me that it&amp;#8217;s chosen the right point in the design space, or that it&amp;#8217;s fulfilling a particularly pressing need for real applications. I think it&amp;#8217;s still interesting to hear about, but I wouldn&amp;#8217;t pick this for a 10 year best paper&amp;nbsp;award.&lt;/p&gt;

   </content></entry><entry><title>Paper review: The Google File System</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/gfs_review.html"/><updated>2011-09-24T15:39:00Z</updated><published>2011-09-24T15:39:00Z</published><id>http://www.umbrant.com/blog/2011/gfs_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review for &amp;#8220;The Google File System&amp;#8221; by Ghemawat et al., published at &lt;span class=&quot;caps&quot;&gt;SOSP&lt;/span&gt; in 2003. This is a fairly important paper, and directly inspired the architecture of&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt;.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;What Google did was look very carefully at their desired workload, and build a distributed filesystem specifically for that. &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; is very much not a general purpose filesystem, and I really like how they lay out quite clearly early on the assumptions they&amp;nbsp;make:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Files are almost all large, many&amp;nbsp;GBs&lt;/li&gt;
&lt;li&gt;Target throughput, not&amp;nbsp;latency&lt;/li&gt;
&lt;li&gt;Append-only. Cannot overwrite existing&amp;nbsp;data.&lt;/li&gt;
&lt;li&gt;Must be distributed and&amp;nbsp;fault-tolerant&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem they were basically trying to solve was doing log analytics at scale, meaning mostly long sequential writes of very large files. Spreading files across multiple disks is crucial to getting enough throughput and getting fault-tolerance. &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; can be viewed sort of like a distributed version of &lt;span class=&quot;caps&quot;&gt;RAID&lt;/span&gt;&amp;nbsp;1.&lt;/p&gt;
&lt;p&gt;The architecture is a single &lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; master which stores metadata for all the files, and a lot of chunkservers that store chunks (&lt;span class=&quot;caps&quot;&gt;64MB&lt;/span&gt;) of files. The master is used to chunk locations for a given range of a file, the actual reads and writes are done by directly accessing the appropriate chunkserver. All chunks are replicated across multiple chunkservers for durability and load balancing. Chunkservers talk to the master via heartbeat messages, upon which the master can piggyback commands like re-replicating or getting chunk&amp;nbsp;lists.&lt;/p&gt;
&lt;p&gt;Data consistency is made a lot easier by not having to worry about overwrites. It also means clients can cache chunk locations, since they change&amp;nbsp;rarely.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;relevance&lt;/h3&gt;
&lt;p&gt;&lt;span class=&quot;caps&quot;&gt;GFS&lt;/span&gt; clearly does a good job at the application it was designed for: sequential reads for large files by data-parallel workloads. However, since &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt; has become sort of an industry standard for storing large amounts of data, it&amp;#8217;s increasingly being used for other types of workloads. HBase is one example of this (a more database-like column store), which definitely does a lot more random I/Os. Facebook also published a paper on doing real-time queries with MapReduce (and thus &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt;). The question is how well &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt; can be squeezed into these roles, and if other storage systems are necessary. For low-latency web serving this is definitely true (memcached and other k-v stores&amp;nbsp;dominate).&lt;/p&gt;
&lt;p&gt;In short, I don&amp;#8217;t think the MapReduce paradigm is going anywhere, and &lt;span class=&quot;caps&quot;&gt;HDFS&lt;/span&gt; already feels like the standard answer to storing big data. I don&amp;#8217;t think it&amp;#8217;s going anywhere in the next&amp;nbsp;decade.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Paxos, Paxos, and Chubby</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/paxos_papers_review.html"/><updated>2011-09-22T14:53:00Z</updated><published>2011-09-22T14:53:00Z</published><id>http://www.umbrant.com/blog/2011/paxos_papers_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;I&amp;#8217;m combining my paper reviews this time, since they are pretty closely&amp;nbsp;coupled:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Paxos Made Simple&amp;#8221;, Lamport,&amp;nbsp;2001&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Paxos Made Practical&amp;#8221;, Mazieres,&amp;nbsp;2007&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;The Chubby Lock Service for Loosely-coupled Distributed Systems&amp;#8221;, Burrows,&amp;nbsp;2006&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Paxos Made&amp;nbsp;Simple&lt;/h3&gt;
&lt;p&gt;Paxos is a distributed consensus algorithm. At it&amp;#8217;s essence, it&amp;#8217;s a quorum-based fault-tolerant way of arriving at a single consistent value among a group of machines. This can be used to do leader election (consensus on who&amp;#8217;s the master), or synchronous strong consistency (replicating writes in a distributed database). In the case of Chubby, it&amp;#8217;s used for both: determining who holds a lock (leader election), and serving strongly consistent small&amp;nbsp;files.&lt;/p&gt;
&lt;p&gt;Paxos has two basic types of&amp;nbsp;actors:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Proposers&lt;/em&gt; are nodes that are trying to get a value accepted as the &amp;#8220;true&amp;#8221; value during a round of consensus. Normally, which node is the proposer is pretty stable, and only changes on&amp;nbsp;failure.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;Acceptors&lt;/em&gt; receive proposals from proposers, and vote as part of the quorum. They act as replicas for the global state of the&amp;nbsp;system.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Paxos also has two rounds of communication. Here I&amp;#8217;m showing the basic version. There are four types of messages here: propose, promise, accept, and&amp;nbsp;accepted.&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Propose&lt;ul&gt;
&lt;li&gt;The proposer sends a &lt;em&gt;proposal&lt;/em&gt; to all the acceptors in the system, and waits for a reply. This proposal is tagged with a round number &lt;strong&gt;N&lt;/strong&gt;, which has to increase each time this proposer makes a new proposal. A common thing to do is make this number out of the &lt;span class=&quot;caps&quot;&gt;IP&lt;/span&gt; address and some local counter, so they are globally&amp;nbsp;unique.&lt;/li&gt;
&lt;li&gt;The acceptors &lt;em&gt;promise&lt;/em&gt; to accept the proposal if the proposal&amp;#8217;s &lt;strong&gt;N&lt;/strong&gt; is the highest &lt;strong&gt;N&lt;/strong&gt; they&amp;#8217;ve seen from any proposer. A promise indicates that the acceptor will ignore any proposal with a lower number. This is so there&amp;#8217;s a total ordering on proposals; we don&amp;#8217;t care which proposer wins, we just want one to&amp;nbsp;win. &lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;Accept&lt;ul&gt;
&lt;li&gt;If the proposer hears back from a quorum of acceptors, it sends out &lt;em&gt;accept&lt;/em&gt; requests to the quorum with the value it wants to&amp;nbsp;set.&lt;/li&gt;
&lt;li&gt;If the acceptor hasn&amp;#8217;t promised to a higher proposal number in the meantime, it tells the proposer it &lt;em&gt;accepted&lt;/em&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If the proposer gets back a quorum of accepted messages, then it commits. At this point, it can further replicate the value to other nodes, or respond to the client that the operation worked. It&amp;#8217;s normal for all nodes in the system to in fact fulfill the roles of both proposer and acceptor as&amp;nbsp;needed.&lt;/p&gt;
&lt;p&gt;There is one detail that prevents future proposers from changing the value when issuing a higher number proposal. In the first round, the acceptor promise also sends the number and value of the highest number proposal they&amp;#8217;ve accepted. Then, the proposer uses the value of the highest number proposal it gets back when it commits in the accept&amp;nbsp;round.&lt;/p&gt;
&lt;h3&gt;Paxos Made&amp;nbsp;Practical&lt;/h3&gt;
&lt;p&gt;This paper goes into a lot of detail about how to actually implement Paxos, handling nodes leaving and adding a Paxos replicated state machine (ala an &lt;span class=&quot;caps&quot;&gt;RPC&lt;/span&gt; server). All nodes run the same deterministic code which transitions based on input. Non-deterministic bits are decided by a single machine externally before being passed into the state machine. One of the machines act as the master (proposer) in Paxos operations, handling a client request, running Paxos, and executing and responding to the client after a majority of nodes have logged the operation. Changes in group membership (the view) are handled by again running Paxos to agree on&amp;nbsp;membership.&lt;/p&gt;
&lt;h3&gt;Chubby Lock&amp;nbsp;Service&lt;/h3&gt;
&lt;p&gt;This is an engineering industry paper from Google, always interesting reads. The basic idea of Chubby is running Paxos on a small &lt;em&gt;cell&lt;/em&gt; of five machines to solve consensus problems. One of the five machines in the cell acts as the master, and runs Paxos to decide writes. Reads are all served from the master. This lets Chubby provide &lt;em&gt;coarse-grained&lt;/em&gt; locking services, where locks are expected to be held for long periods of time (minutes, hours, days). Finer grained locking is deferred to application-level servers. Chubby&amp;#8217;s &lt;span class=&quot;caps&quot;&gt;API&lt;/span&gt; is through a simple way Unix-like filesystem, upon which clients can open file handles and get and release reader/writer locks. This is also convenient for advertising results, and can be used to store small files in a very consistent manner. A typical use within Google is using it as a better version of &lt;span class=&quot;caps&quot;&gt;DNS&lt;/span&gt; (no worrying about stale entries and&amp;nbsp;TTLs).&lt;/p&gt;
&lt;p&gt;The rest of the paper describes other features of Chubby: failover, cache consistency, event notifications. Master failover is handled by leader election in the Chubby cell, with a new node brought online in the background. Client caching is also an essential part of reducing load on the Chubby master, but the master has to synchronously invalidate caches on writes. Invalidation is piggybacked on heartbeat &lt;em&gt;KeepAlive&lt;/em&gt; messages (default every 12 seconds), a lease mechanism that keeps client locks and cache alive. Event notifications can also be used to watch files for certain events: modification, master failover, etc. This is most often used to wait for modifications to a file, indicating that a new leader has locked and written its address to the&amp;nbsp;file.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;trends&lt;/h3&gt;
&lt;p&gt;Seeing how this is being used to great extent at Google, it&amp;#8217;s got great future applicability. Chubby seems to be used mostly for inter-datacenter locking, I wonder if there are any important modifications that have to be done for intra-datacenter locking. The whole space of eventual consistency gives a lot of alternatives to the strong guarantees that Paxos offers, so there are lots of ways to tradeoff availability and performance with&amp;nbsp;consistency.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Cluster-Based Scalable Network Services</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/cluster_based_scalable_network_services_review.html"/><updated>2011-09-22T14:01:00Z</updated><published>2011-09-22T14:01:00Z</published><id>http://www.umbrant.com/blog/2011/cluster_based_scalable_network_services_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;Cluster-Based Scalable Network Services&amp;#8221; by Fox et al., published at &lt;span class=&quot;caps&quot;&gt;SOSP&lt;/span&gt; in 1997. It describes an architecture for datacenter services that proved to be prescient, and used the Inktomi as an&amp;nbsp;example.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h2&gt;Main&amp;nbsp;ideas&lt;/h2&gt;
&lt;p&gt;This paper has to be put into context. At this point there was still contention whether &amp;#8220;clusters of workstations&amp;#8221; was the right approach for handling web-sized workloads. Inktomi was at the forefront of saying that yes, clusters were the right choice, and this paper demonstrates why this is true, and how datacenter services can be structured to achieve their key goals: &lt;em&gt;scalability&lt;/em&gt;, &lt;em&gt;availability&lt;/em&gt;, and &lt;em&gt;cost effectiveness&lt;/em&gt; by using consistency semantics weaker than &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt;:&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;BASE&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The advantages of clusters are manyfold. They allow easy incremental scaling and upgrading, they can be build out of commodity parts, and they have natural redundancy through replication. Disadvantages are primarily in the programming model and management; it can be difficult to harness a group of machines to complete a task, and since it&amp;#8217;s a distributed system, there are issues with data consistency and&amp;nbsp;failures.&lt;/p&gt;
&lt;p&gt;The idea of &lt;span class=&quot;caps&quot;&gt;BASE&lt;/span&gt; is a crucial component of this. &lt;span class=&quot;caps&quot;&gt;BASE&lt;/span&gt; stands for &lt;strong&gt;B&lt;/strong&gt;asically &lt;strong&gt;A&lt;/strong&gt;vailable, &lt;strong&gt;S&lt;/strong&gt;oft state, &lt;strong&gt;E&lt;/strong&gt;ventual consistency. This is a significant relaxation of strict &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt; semantics, since it allows servers to temporarily serve stale data while state converges. This is allows better performance, and many applications do not require strict &lt;span class=&quot;caps&quot;&gt;ACID&lt;/span&gt; semantics to provide a good user&amp;nbsp;experience.&lt;/p&gt;
&lt;p&gt;The cluster architecture proposed also looks shockingly similar to what is in use today. Within a datacenter, machines are split into two major groups: front-end and workers. Front-ends handle actual client requests from outside the datacenter. To handle a client request, a front-end might harness a number of workers running different services to get data or do computation, before assembling and returning the response. This allows all the front-ends to share from the same pool of stateless workers which is good for utilization, and also allows pools of workers to be scaled up and down in response to&amp;nbsp;overload.&lt;/p&gt;
&lt;h2&gt;Future trends and&amp;nbsp;relevance&lt;/h2&gt;
&lt;p&gt;Seeing how Brewer wrote this paper in 1997 and we&amp;#8217;re still using roughly the same architecture today in 2011, I don&amp;#8217;t think there&amp;#8217;s any doubt that the paper had a lot of future relevance. I think there&amp;#8217;s still room for improvement in the cluster management side of things (Mesos), but the idea of clusters for datacenters has reached complete acceptance. Interestingly though, we&amp;#8217;re seeing the return of &amp;#8220;big iron&amp;#8221; to the datacenter for some applications. People are starting to wonder about the possibilities offered by a machine with &lt;span class=&quot;caps&quot;&gt;1TB&lt;/span&gt; of memory (purchasable today), and the &amp;#8220;disk is tape, memory is disk&amp;#8221; argument along with a strong focus on latency might lead to further development on the cluster programming model front. SSDs present yet another level in the storage hierarchy with unique cost and performance&amp;nbsp;tradeoffs.&lt;/p&gt;

   </content></entry><entry><title>Paper review: The Datacenter Needs an Operating System</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/datacenter_needs_an_operating_system_review.html"/><updated>2011-09-14T11:40:00Z</updated><published>2011-09-14T11:40:00Z</published><id>http://www.umbrant.com/blog/2011/datacenter_needs_an_operating_system_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;The Datacenter Needs an Operating System&amp;#8221; by Zaharia et al. This is a short 5 page paper published at HotCloud&amp;nbsp;2011.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;This is a high-level ideas paper focusing on the abstractions that should be provided in a cluster programming environment. The authors identify the following as the core traits of traditional operating&amp;nbsp;systems:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Resource&amp;nbsp;sharing&lt;/li&gt;
&lt;li&gt;Data sharing between&amp;nbsp;programs&lt;/li&gt;
&lt;li&gt;Programming abstractions for software&amp;nbsp;development&lt;/li&gt;
&lt;li&gt;Debugging and&amp;nbsp;monitoring&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The authors argue that these same things should be provided to cluster applications as a common layer, instead of having each programming paradigm separately implement them in an adhoc fashion. If I interpret the article correctly, they want to provide a common set of abstractions on top of which programming models like MapReduce or Dryad can be built, benefiting from code sharing as well as more efficient&amp;nbsp;utilization.&lt;/p&gt;
&lt;h3&gt;Problems&lt;/h3&gt;
&lt;p&gt;What I really would have liked to have seen (perhaps in a longer paper) is more of a focus on where the abstractions are going to be drawn between the &amp;#8220;datacenter &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt;&amp;#8221; and the &amp;#8220;datacenter application&amp;#8221;. This is a classic problem in traditional &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; literature (should it be in the kernel, or in user space?), and I&amp;#8217;m betting we&amp;#8217;ll see the same theme being explored here. Since all these different frameworks already exist, the initial question is figuring out what can actually be pushed down into the&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;Debugging is one issue raised that none of the computing frameworks have really solved, so that&amp;#8217;s the least-defined problem on the list in my mind. Scheduling and resource sharing are difficult, but&amp;nbsp;approachable.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;impact&lt;/h3&gt;
&lt;p&gt;Highly relevant. A proper datacenter operating system enables higher utilization, better performance, and an easier programming environment. As stated before, cloud computing is only becoming more prevalent and&amp;nbsp;important.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Performance Modeling and Analysis of Flash-based Storage Devices</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/performance_modeling_and_analysis_of_flash_review.html"/><updated>2011-09-12T23:43:00Z</updated><published>2011-09-12T23:43:00Z</published><id>http://www.umbrant.com/blog/2011/performance_modeling_and_analysis_of_flash_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;Performance Modeling and Analysis of Flash-based Storage Devices&amp;#8221; by Huang et al. This paper compares and contrasts three different types of drives: a high-end Intel &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;, a low-end Samsung &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;, and a 5400 &lt;span class=&quot;caps&quot;&gt;RPM&lt;/span&gt; Samsung &lt;span class=&quot;caps&quot;&gt;HDD&lt;/span&gt;. My review focuses on the implications for cloud&amp;nbsp;computing.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;p&gt;Solid state disks have to be treated kind of like black boxes when it comes to performance modeling. As is evident from comparing the Intel and Samsung SSDs, we can get wildly different performance characteristics depending on how the manufacturer has configured the Flash Translate Layer (&lt;span class=&quot;caps&quot;&gt;FTL&lt;/span&gt;), which maps block requests to actual storage locations. This is important because the &lt;span class=&quot;caps&quot;&gt;FTL&lt;/span&gt; acts as a pseudo-filesystem, and has the ability to decide how data is really laid out on the drive independent of the ordering presented to the operating system. This has deep implications for filesystem design (don&amp;#8217;t bother minimizing seeks with reordering), and other characteristics of SSDs mean that coalescing and batching requests simply aren&amp;#8217;t as&amp;nbsp;effective.&lt;/p&gt;
&lt;p&gt;It seems really worthwhile for &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; vendors to invest in better &lt;span class=&quot;caps&quot;&gt;FTL&lt;/span&gt; code, since the high-end Intel &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; appears almost unaffected performance-wise by the randomness of its workload. This indicates to me that it&amp;#8217;s probably doing &lt;span class=&quot;caps&quot;&gt;LFS&lt;/span&gt;-like block placement to try and minimize the number of writes and erases. This is in stark contrast to the Samsung &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;, which sees its random write performance get trounced by&amp;nbsp;sequential.&lt;/p&gt;
&lt;p&gt;I think there&amp;#8217;s definitely space for SSDs in the storage hierarchy on the cloud. It&amp;#8217;s safe to assume future SSDs will have &lt;span class=&quot;caps&quot;&gt;FTL&lt;/span&gt; firmware with performance similar to the Intel &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;, so we&amp;#8217;re basically gaining a tier that has strictly better performance characteristics than hard disk drives. It won&amp;#8217;t displace HDDs entirely since the price/&lt;span class=&quot;caps&quot;&gt;GB&lt;/span&gt; is still way higher and the amount of data is only growing, but it does sit sort of nicely between memory and HDDs. There&amp;#8217;s less of a value argument for MapReduce (an application designed specifically to make good use of HDDs by doing large sequential reads), but for more random workloads (thinking of &lt;span class=&quot;caps&quot;&gt;OLTP&lt;/span&gt;, or web caches) SSDs will be a big win. The limitations on the number of write cycles could be a problem, but can be countered through schemes like unbalancing writes to a single &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; in an array (making it fail first reliably, which is better), or by ironically sticking a &lt;span class=&quot;caps&quot;&gt;HDD&lt;/span&gt; in front to buffer&amp;nbsp;writes.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Data-Level Parallelism in Vector, SIMD, and GPU Architectures</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/data-level_parallelism_review.html"/><updated>2011-09-12T23:21:00Z</updated><published>2011-09-12T23:21:00Z</published><id>http://www.umbrant.com/blog/2011/data-level_parallelism_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a review of Chapter 4 from the Hennessy and Patterson book, &amp;#8220;Computer Architecture, 5th Edition: A Quantitative Approach&amp;#8221;. This chapter covers the differences between vector processors, &lt;span class=&quot;caps&quot;&gt;SIMD&lt;/span&gt;, and &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; architectures. This writeup focuses entirely on the future hardware trends of data-parallel &lt;span class=&quot;caps&quot;&gt;SIMD&lt;/span&gt; hardware in the&amp;nbsp;cloud.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;p&gt;My claim is that we&amp;#8217;re going to see GPUs moving on-chip to coexist with normal &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; cores; we&amp;#8217;re already seeing this happen with the newest processors from Intel and &lt;span class=&quot;caps&quot;&gt;AMD&lt;/span&gt;. The reason for this is that the latency cost in shifting computation to a &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; over &lt;span class=&quot;caps&quot;&gt;PCI&lt;/span&gt;-E is high, and there isn&amp;#8217;t any memory coherency. Moving &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; cores on-chip solves both of these problems, though memory coherency still faces the same problems it does among many &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; cores. I can&amp;#8217;t make any solid predictions about vector processors, but it seems like the momentum is heavily in favor of &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; and &lt;span class=&quot;caps&quot;&gt;SIMD&lt;/span&gt; &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; extensions for data-parallel&amp;nbsp;computation.&lt;/p&gt;
&lt;p&gt;How does this relate to cloud computing? GPUs face one major problem in a cloud environment, in that they are currently quite hard to virtualize. Preemption support is nascent, and there&amp;#8217;s a high cost to context switching a &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; because of the high latency bus. Concurrent sharing might also be difficult. There also aren&amp;#8217;t well-defined standards for programming a &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; (&lt;span class=&quot;caps&quot;&gt;CUDA&lt;/span&gt; and OpenCL being competing&amp;nbsp;examples).&lt;/p&gt;
&lt;p&gt;Putting all of the problems aside as solvable however, GPUs are great for speeding up data-parallel tasks (and GPUs already can run MapReduce). I definitely see them gaining traction for batch processing. For normal web-serving workloads though, I&amp;#8217;m not sure where a &lt;span class=&quot;caps&quot;&gt;GPU&lt;/span&gt; would be useful. I think it&amp;#8217;s a lot harder to derive data-parallelism from handling a single request, and the current limitations on multiplexing and latency make it less friendly for this type of&amp;nbsp;work.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Amdahl&#39;s Law in the Multicore Era</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/amdahls_law_review.html"/><updated>2011-09-11T19:08:00Z</updated><published>2011-09-11T19:08:00Z</published><id>http://www.umbrant.com/blog/2011/amdahls_law_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a paper review of &amp;#8220;Amdahl&amp;#8217;s Law in the Multicore Era&amp;#8221;, by Hill and&amp;nbsp;Marty.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main&amp;nbsp;ideas&lt;/h3&gt;
&lt;p&gt;There are two important equations in this paper that lay the foundation for the rest of the paper: Amdahl&amp;#8217;s law, and the processor performance&amp;nbsp;law.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;speedup = 1 / ((1-f + f/s)
perf(r) = sqrt(r)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;These two equations have some deep implications. Starting with the latter, there are diminishing returns from investing more chip resources to single core performance. It&amp;#8217;s the reason why per-core performance has basically peaked in recent years, and everything is moving toward multicore. Adding more cache and beefing up adders can only do so much; since clock rates have peaked because of power limitations, we&amp;#8217;re basically stuck with the per-core performance we&amp;#8217;ve&amp;nbsp;got.&lt;/p&gt;
&lt;p&gt;Moving on to Amdahl&amp;#8217;s law, &lt;code&gt;f&lt;/code&gt; is the fraction of a program that is parallelizable, and &lt;code&gt;s&lt;/code&gt; is the speedup of the parallelizable part. Interpreting this, we see there are essentially three ways of making your program run&amp;nbsp;faster:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run the serial part faster (&lt;code&gt;1-f&lt;/code&gt; component)&lt;/li&gt;
&lt;li&gt;Make &lt;code&gt;s&lt;/code&gt; bigger (increase the granularity of&amp;nbsp;parallelism)&lt;/li&gt;
&lt;li&gt;Make &lt;code&gt;f&lt;/code&gt; bigger (parallelize serial parts of the&amp;nbsp;code)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Combining both equations, what are the limitations we&amp;nbsp;see?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Running the serial part faster gets expensive quickly in terms of chip&amp;nbsp;resources.&lt;/li&gt;
&lt;li&gt;Increasing the granularity of parallelism is a great approach, but only works really well for data-parallel tasks like serving webpages. Going beyond this natural parallelism into true fine-grained parallelism is expensive and&amp;nbsp;limited.&lt;/li&gt;
&lt;li&gt;Parallelizing serial parts of the code is also a great approach, but similarly limited. Some serial code simply can&amp;#8217;t be parallelized, and there&amp;#8217;s the same diminishing returns effect in terms of programmer&amp;nbsp;time.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The article goes on to talk about different ways of allocating chip resources in light of these two laws, covering the symmetric, asymmetric, and dynamic multicore chips. Basically, favoring bigger cores makes the serial part run faster, but reduces the number of ways you can split the parallel part, with smaller cores having the opposite effect. 
The net result of the paper is that it all depends on your workload. Asymmetric and dynamic multicore offer better performance characteristics than symmetric since they can better handle both the serial and parallel parts of a program&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;Applying this to cloud computing, we see that there are a lot of parallels between multicore and cloud computing. Instead of getting speedup by parallelizing at the level of instructions in a program, web services are typically scaled through request-level parallelism: distributing requests among a cluster of machines. In this case, the &lt;code&gt;1-f&lt;/code&gt; factor in Amdahl&amp;#8217;s law can be effectively zero, since requests can be handled independently by each&amp;nbsp;machine.&lt;/p&gt;
&lt;p&gt;The ideas of asymmetric cores is also already seen in request-level parallelism since bigger tasks can be allocated to bigger&amp;nbsp;machines.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;trends&lt;/h3&gt;
&lt;p&gt;Servers processors are going to be manycore in the future, whether we want it or not. Peak per-core performance is probably not going to increase, and I don&amp;#8217;t hold out much hope for the effectiveness of dynamic multicore, but I think there&amp;#8217;s a strong argument for asymmetric multicore since there are demonstrated benefits over symmetric. Big cores will be used for serial computation, with data-parallel parts off-loaded to small cores as possible. This is also better for the coming scarcity of memory bandwidth; having big cores that can use it more effectively makes&amp;nbsp;sense.&lt;/p&gt;
&lt;p&gt;This also lends flexibility into request handling, since high-priority tasks can get the big core while latency-tolerant operations can run on a small core. The big problem here is going to be performance isolation for shared resources like mem, disk, and network; with more threads, there&amp;#8217;s more contention, and potentially more variability in performance (which is the killer). Scheduling also becomes more difficult, since &amp;#8220;slots&amp;#8221; on the same machine are now unequal in&amp;nbsp;size.&lt;/p&gt;
&lt;p&gt;For batch processing (like Hadoop), throughput matters more than latency. Here, &lt;span class=&quot;caps&quot;&gt;SMP&lt;/span&gt; is the name of the game: we want to optimize &lt;code&gt;perf(r)&lt;/code&gt; and have a bunch of wimpy cores. I still worry about I/O demands on durable storage, since &lt;span class=&quot;caps&quot;&gt;HDD&lt;/span&gt; throughput doesn&amp;#8217;t seem to be increasing at the same rate that cores are being&amp;nbsp;added.&lt;/p&gt;
&lt;p&gt;Software stack&amp;nbsp;predictions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;We&amp;#8217;re going to see a focus on better asymmetric-aware, load-based cluster scheduling algorithms. This needs to balance out I/O load&amp;nbsp;too.&lt;/li&gt;
&lt;li&gt;We&amp;#8217;re going to see moderate fine-grained parallelism added to existing request-level parallelism, since it&amp;#8217;s the real only way of reducing&amp;nbsp;latency. &lt;/li&gt;
&lt;li&gt;As a corollary, I think there&amp;#8217;s going to be a big focus on performance isolation even at the cost of throughput, since variation in latency is the real&amp;nbsp;killer.&lt;/li&gt;
&lt;li&gt;MapReduce as a programming model and framework probably will not go away, but we&amp;#8217;re going to see mappers making better use of multi-core. Whether this is going to be MapReduce-in-MapReduce or OpenMP-in-MapReduce or Pthreads-in-MapReduce is unclear, but it makes sense to further break down an already data-parallel task into core-sized&amp;nbsp;chunks.&lt;/li&gt;
&lt;/ul&gt;

   </content></entry><entry><title>Paper review: The Datacenter as a Computer Ch. 3, 4, 7</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/datacenter_as_a_computer_3_4_7_review.html"/><updated>2011-09-08T20:31:00Z</updated><published>2011-09-08T20:31:00Z</published><id>http://www.umbrant.com/blog/2011/datacenter_as_a_computer_3_4_7_review.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a review of chapters 3, 4, and 7 from &amp;#8220;The Datacenter as a Computer&amp;#8221;. The topics covered are hardware for servers, datacenter basics, and fault-tolerance and&amp;nbsp;recovery. &lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Main&amp;nbsp;idea&lt;/h3&gt;
&lt;p&gt;The three chapters here cover essentially how to design the hardware, software, and operational concerns of a&amp;nbsp;datacenter. &lt;/p&gt;
&lt;p&gt;In terms of datacenter hardware, the main concern is choosing the most cost-efficient type of node that still runs your workload sufficiently&amp;nbsp;fast.&lt;/p&gt;
&lt;p&gt;Operationally, the concerns deal with power and cooling of nodes, factors which limit the density of nodes in a datacenter. Cooling can account for a major part of the power cost of a datacenter, and &lt;span class=&quot;caps&quot;&gt;AC&lt;/span&gt; is just as critical a service as power since the datacenter can survive for only a matter of minutes if the &lt;span class=&quot;caps&quot;&gt;AC&lt;/span&gt; unit&amp;nbsp;dies.&lt;/p&gt;
&lt;p&gt;The chapter on fault-tolerance and recovery talks about the different types of faults that can present in hardware and software, and how they might affect service availability. The ultimate goal of the service is to be able to survive faults without significantly affecting availability, either through overprovisioning or graceful degradation of service&amp;nbsp;quality.&lt;/p&gt;
&lt;h3&gt;Problems&amp;nbsp;presented&lt;/h3&gt;
&lt;p&gt;I divided this up into three sections, based on the&amp;nbsp;chapters.&lt;/p&gt;
&lt;h4&gt;Hardware/software&amp;nbsp;scaling&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;How low can you go? Small nodes are more cost-efficient than beefy ones in terms of computation-per-watt and price-per-computation, but might start becoming a bottleneck due to the limits of request parallelism. Further parallelization of an application can be really painful, and having to deal with coordination between more nodes has its drawbacks. Never forget Amdahl&amp;#8217;s Law: if the serial parts of your program dominate, and you&amp;#8217;re running that serial code on slow nodes, your program is going to run slowly too. Beefy nodes can run the serial part&amp;nbsp;quickly.&lt;/li&gt;
&lt;li&gt;Another point about small nodes is that they are harder to schedule efficiently. Resources effectively get fragmented; there might not be enough left on a small node to schedule a new task. Big nodes pack more&amp;nbsp;efficiently.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4&gt;Operations&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;What is the most effective way of cooling servers? This is heavily related to power density (# of servers / volume), with a higher power density being a more efficient use of a datacenter. The current canonical strategy seems to be alternating hot and cold aisles, with cold air pumped up through the floor by a central &lt;span class=&quot;caps&quot;&gt;AC&lt;/span&gt; unit. Modular container datacenters seem to be another strategy, which are effective because they can be designed very tightly and are&amp;nbsp;self-contained.&lt;/li&gt;
&lt;li&gt;What is the most efficient way of cooling server? This relates to economic costs; it&amp;#8217;s claimed that cooling can account for 40% of load, which is a big power&amp;nbsp;bill.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One question I had here was that they mention that UPSes are generally in a separate room, with just the power distribution unit on the floor. I remember reading that Google integrated batteries right into their servers, but maybe one source or the other is outdated. Having a battery right in the server might increase fault-tolerance and&amp;nbsp;modularity.&lt;/p&gt;
&lt;h4&gt;Fault-tolerance&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;What are the real limitations on availability? Internet is apparently only 99.99% available, and software faults dominate hardware faults (only 10%). Machines apparently last an average of 6 years before replacement, after factoring out &amp;#8220;infant mortality&amp;#8221;, a number I found to be surprisingly high considering how much talk there is of nodes keeling over, but again just indicates that most keeling is due to software&amp;nbsp;faults.&lt;/li&gt;
&lt;li&gt;How do you maintain service when faults happen? In a datacenter, a new node fails on the order of hours, so it&amp;#8217;s not acceptable to become unavailable. This is done through some degree of overprovisioning of resources, and designing software that is fault-tolerant. When operating with faults, it&amp;#8217;s desirable to have the property of &lt;em&gt;graceful degradation&lt;/em&gt;, where the quality of service gradually degrades (e.g. using older cached data, serving reads but denying writes, disabling some features). Fault-tolerance also makes the need for repairs less urgent and thus less&amp;nbsp;expensive.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Tradeoffs&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Programming cost vs. hardware cost. Can speed up either by throwing programmers at a problem (squeeze more parallelism from the code), or by buying better hardware (run the same code faster). This is an economic balancing&amp;nbsp;act.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Impact&lt;/h3&gt;
&lt;p&gt;This textbook isn&amp;#8217;t going to be winning any awards, but I find it to be a fascinating look into operations and system design at Google. Like I mentioned in my last writeup, there are only going to be more datacenters being built in the future, so advice like this on how to design and build datacenters and datacenter applications is very&amp;nbsp;useful.&lt;/p&gt;

   </content></entry><entry><title>Paper review: Warehouse-Scale Computing: Entering the Teenage Decade</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/warehouse_scale_computing_summary.html"/><updated>2011-09-06T22:10:00Z</updated><published>2011-09-06T22:10:00Z</published><id>http://www.umbrant.com/blog/2011/warehouse_scale_computing_summary.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This review is based on a presentation by Luiz Andre Barroso from Google, titled &amp;#8220;Warehouse-Scale Computing: Entering the Teenage Decade&amp;#8221;. I believe it was given this past year (2011) at &lt;span class=&quot;caps&quot;&gt;FCRC&lt;/span&gt;. I really strongly recommend it, since it talks about the operational issues in running a Google datacenter, and also identifies a lot of the research issues surrounding &amp;#8220;warehouse-scale&amp;nbsp;computation&amp;#8221;.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Key&amp;nbsp;Points&lt;/h3&gt;
&lt;p&gt;There were a couple problems identified in the talk, some of which have been solved, some of which have not&amp;nbsp;been.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;I/O latency variability right now is terrible, with basically all durable storage displaying a long latency tail. Random accesses to spinning disks are slow, flash writes are slow, and these high-latency events muck up the latency for potentially fast&amp;nbsp;events.&lt;/li&gt;
&lt;li&gt;Network I/O suffers a similar problem. Using &lt;span class=&quot;caps&quot;&gt;TCP&lt;/span&gt; and interrupts adds orders of magnitude of latency to network requests, making fast network hardware slow again in&amp;nbsp;software.&lt;/li&gt;
&lt;li&gt;Datacenter power efficiency as defined by &lt;span class=&quot;caps&quot;&gt;PUE&lt;/span&gt; (Power Usage Effectiveness) has gotten pretty good (&amp;lt;10 percent is used on operational overhead). The real problem now is making better use of servers, to get &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; load up into the 80% range instead of the current&amp;nbsp;30%.&lt;/li&gt;
&lt;li&gt;This leads into the another problem: how do you share all of the resources in a cluster among many different services, while also chopping off the latency&amp;nbsp;tail?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To summarize, there are two big ideas in the talk. First, latency and variation in latency are the key performance metrics for services these days; today&amp;#8217;s web-based applications demand both to provide a good user experience. This may require reexamination of a lot of fundamental assumptions about &lt;span class=&quot;caps&quot;&gt;IO&lt;/span&gt;. Second, increasing the utilization of resources in a cluster is important from an efficiency and performance standpoint. Server hardware should be a fungible resource that can be easily shared among different&amp;nbsp;services.&lt;/p&gt;
&lt;p&gt;The differences here between warehouse-scale computing and datacenter-scale computing lie in scale and the type of user experience provided. Warehouse-scale computing operates in the many petabyte range, and allows for complete system integration of the hardware, software, power, and cooling of the cluster. The types of services hosted by a warehouse-scale computer are also scaled way up, in terms of the latency requirements and the size of the data that is being&amp;nbsp;crunched.&lt;/p&gt;
&lt;h3&gt;Trade&amp;nbsp;offs&lt;/h3&gt;
&lt;p&gt;One of the key tradeoffs mentioned was between latency and throughput. Most of the software stack for I/O these days is done to optimize throughput, done in response to relatively slow disks or networks. An example of this would be Nagle&amp;#8217;s algorithm in &lt;span class=&quot;caps&quot;&gt;TCP&lt;/span&gt;; small packets are delayed and batched to be sent in bulk, reducing &lt;span class=&quot;caps&quot;&gt;TCP&lt;/span&gt; overhead (fewer bytes need to be sent) but also increasing latency. New technologies like flash and fast networks mean that these assumptions should be&amp;nbsp;reexamined.&lt;/p&gt;
&lt;h3&gt;Long-term&amp;nbsp;impact&lt;/h3&gt;
&lt;p&gt;Web-based services are here to stay, and I feel confident in saying that this is an area that is going to see yet more growth. Large internet companies like Google and Facebook are already dealing with these issues internally, and there are only going to be more warehouse-scale datacenters built. It&amp;#8217;s clear that these are hard problems that aren&amp;#8217;t going away because of some &lt;em&gt;deus ex machina&lt;/em&gt; like Moore&amp;#8217;s Law, so any solutions are likely to have a big&amp;nbsp;impact.&lt;/p&gt;

   </content></entry><entry><title>Lottery and stride scheduling</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/lottery_stride_scheduling.html"/><updated>2011-07-17T02:50:00Z</updated><published>2011-07-17T02:50:00Z</published><id>http://www.umbrant.com/blog/2011/lottery_stride_scheduling.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;Today is a shorter post than previous topics, since I didn&amp;#8217;t want to lump the last paper (Paxos, ick) in with these. I&amp;#8217;m covering lottery and stride scheduling, two very related approaches to doing efficient proportional-share scheduling. I believe this is the canonical way of doing things, since mClock (by Gulati et al., presented at &lt;span class=&quot;caps&quot;&gt;OSDI&lt;/span&gt; 2010) used stride scheduling successfully to schedule disk I/O in a&amp;nbsp;hypervisor.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Lottery Scheduling: Flexible Proportional-Share Resource Management&amp;#8221;, Waldspurger and Weihl,&amp;nbsp;1994&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Stride Scheduling: Deterministic Proportional-Share Resource Management&amp;#8221;, Waldspurger and Weihl,&amp;nbsp;1995&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;The basic problem for proportional-share scheduling is, given a set of processes that have been assigned relative &lt;em&gt;weights&lt;/em&gt; (e.g. 3:2:1), schedule the processes with some kind of quantum such that they all get their assigned proportion of &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; time (e.g. 1/2rd, 1/3rd, 1/6th). A naive way of doing this is to simply schedule each process for &lt;em&gt;weight&lt;/em&gt; number of scheduling quantums, but this penalizes processes that do not use their entire allocation (for instance, if they block on I/O early) and results in unfairness at small timescales (think of a 100:1:1 weighting). It&amp;#8217;s also desired that scheduling is responsive to changes in&amp;nbsp;priorities.&lt;/p&gt;
&lt;p&gt;These are some of the practical problems that any good proportional share scheduler has to&amp;nbsp;solve.&lt;/p&gt;
&lt;h3&gt;Lottery&amp;nbsp;Scheduling&lt;/h3&gt;
&lt;p&gt;Waldspurger and Weihl&amp;#8217;s first approach is a probabilistically fair one. Processes are assigned a number of &lt;em&gt;tickets&lt;/em&gt; based on their relative weight (bigger weight=more tickets), and the scheduler holds a &lt;em&gt;lottery&lt;/em&gt; each scheduling quantum (choosing a random ticket) where the winner gets scheduled. A quantum is small unit of time, in this case 10ms, which is the smallest unit that the scheduler will assign to a process. This means processes with more tickets get scheduled more often, and over time the actual scheduling should probabilistically approaches the desired relative weighting between the processes with standard deviation proportional to &lt;code&gt;sqrt(n)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This can be implemented by logically storing the number of tickets of each processes in an array, and keeping a running sum of tickets as one traverses the array, advancing until the winning ticket is found. This is an &lt;code&gt;O(n)&lt;/code&gt; operation, but sorting the array in descending order with insertion sort (or, as the authors do it, the &amp;#8220;move to front&amp;#8221; heuristic) can make this a lot faster. For large n, it&amp;#8217;s better to take an approach with a tree and binary search toward the winning leaf node&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;If a process ends early or runs over its quantum, the size of its ticket pool gets adjusted until it gets scheduled again with a &lt;em&gt;compensation ticket&lt;/em&gt;. This compensation ticket is valued at &lt;code&gt;(1/f)*num_tickets&lt;/code&gt;, where &lt;code&gt;f&lt;/code&gt; is the fraction of its allocated time that it actually used. This will increase or decrease its likelihood of getting scheduled&amp;nbsp;appropriately.&lt;/p&gt;
&lt;p&gt;It&amp;#8217;s relatively straightforward to build a hierarchical scheduling system via what the authors term &lt;em&gt;ticket currencies&lt;/em&gt;: different types of tickets that are backed by other tickets. In this way, groups of processes can be weighted based on one currency, and then the members of each group weighted based on another currency. This is basically just a nice management feature; in the end, everything gets translated back into the base&amp;nbsp;currency.&lt;/p&gt;
&lt;p&gt;They also have this idea of letting clients pass their ticket allocations off to a server, which is kind of cool when combined with a microkernel where everything is a server. It means you can give tickets only to clients, and make it so all server requests have to be paid for with a ticket allocation. Then, tickets accurately capture all work done in the system on behalf of a client process. My random&amp;nbsp;idea.&lt;/p&gt;
&lt;p&gt;They also have a similar idea with a priority inheritance scheme of sorts for locks, where all the tickets of processes waiting on a lock are passed to the process currently holding the lock. They also test lottery scheduling lock acquisition, which seems of somewhat lower utility to me. I don&amp;#8217;t know how often I would ever want to use this, as a&amp;nbsp;programmer.&lt;/p&gt;
&lt;p&gt;Lottery scheduling isn&amp;#8217;t really that responsive to dynamic changes, since it takes time to converge. Its probabilistic nature also means that it can&amp;#8217;t really give predictable performance, meaning that it might be decently suited for &amp;#8220;average throughput&amp;#8221; schedulers, but probably sucks for interactive uses where you want &lt;span class=&quot;caps&quot;&gt;SLA&lt;/span&gt;-like performance&amp;nbsp;guarantees.&lt;/p&gt;
&lt;h3&gt;Stride&amp;nbsp;Scheduling&lt;/h3&gt;
&lt;p&gt;The follow up paper the year after solves many of the problems with lottery scheduling, by introducing the concept of &lt;em&gt;stride scheduling&lt;/em&gt;, which has all the benefits of lottery scheduling while also being deterministic, responsive to dynamic priorities, and better error properties. This is slightly different from how it&amp;#8217;s presented in the paper, but I think it makes more&amp;nbsp;sense.&lt;/p&gt;
&lt;p&gt;This is something most easily explained visually, but the basic idea is the concept of &lt;em&gt;virtual time&lt;/em&gt;, where each process has a clock that ticks at a different rate depending on its priority. High priority clocks tick slowly, while low priority clocks tick quickly. The rate at which a clock ticks is called its &lt;em&gt;stride&lt;/em&gt;. The scheduler makes decisions by finding the clock with the oldest time, scheduling the corresponding process, and then advancing the clock by the clock&amp;#8217;s stride. This is implemented simply by keeping the clocks in ascending sorted order via a heap, insertion sort, or something like&amp;nbsp;that.&lt;/p&gt;
&lt;p&gt;Fractional quantums are handled via a multiplicative compensation factor: simply multiplying the stride by the fraction of the quantum used, before advancing the local&amp;nbsp;clock.&lt;/p&gt;
&lt;p&gt;There is also the concept of &lt;em&gt;global virtual time&lt;/em&gt; that advances at the rate of the slowest possible tick (&lt;code&gt;~1/sum(tickets)&lt;/code&gt;). This is used to calculate a compensation factor, &lt;code&gt;remain&lt;/code&gt;, used to compensate a process for time spent waiting when there is a dynamic change. &lt;code&gt;remain&lt;/code&gt; is the amount of virtual time until a process would next be scheduled, i.e. the difference between the global virtual time and the local virtual time. When the process re-enters the system, its local time is set to &lt;code&gt;global_time+remain&lt;/code&gt;. In this way, if the process waited to be allocated before leaving (&lt;code&gt;remain&amp;amp;lt;stride&lt;/code&gt;) it gets scheduled sooner. The opposite happens if the process previously got an early&amp;nbsp;allocation.&lt;/p&gt;
&lt;p&gt;Since the stride and &lt;code&gt;remain&lt;/code&gt; are both related to the size of the ticket allocation, in the case of a dynamic change, the stride is recomputed and used to scale &lt;code&gt;remain&lt;/code&gt; appropriately to immediate reflect the new&amp;nbsp;allocation. &lt;/p&gt;
&lt;p&gt;This is extremely predictable since scheduling is deterministic, and processes are guaranteed to be scheduled at least once every complete cycle of virtual time (where a cycle is the slowest stride). This is a major boon for responsiveness since we no longer have to wait for probabilities to converge. Glancing at the evaluation section, I see major improvements to predictability, responsiveness to priority changes, and accuracy. The same ideas of ticket currencies and ticket passing also apply to stride&amp;nbsp;scheduling.&lt;/p&gt;

   </content></entry><entry><title>Concurrency review</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/concurrency.html"/><updated>2011-07-08T19:14:00Z</updated><published>2011-07-08T19:14:00Z</published><id>http://www.umbrant.com/blog/2011/concurrency.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;I assume that everyone has already read Andrew Birrell&amp;#8217;s &lt;a href=&quot;ftp://apotheca.hpl.hp.com/pub/dec/SRC/research-reports/SRC-035.pdf&quot;&gt;seminal paper on &amp;#8220;Programming with Threads&amp;#8221;&lt;/a&gt; or at least has a basic conception of parallel programming. This is going to deal with locking and concurrency at a higher level. At-bat today are five selected papers on&amp;nbsp;concurrency:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Granularity of Locks and Degrees of Consistency in a Shared Data Base&amp;#8221;, by Gray et al.,&amp;nbsp;1975&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Experience with Processes and Monitors in Mesa&amp;#8221;, Lampson and Redell,&amp;nbsp;1980&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;On Optimistic Methods for Concurrency Control&amp;#8221;, Kung and Robinson,&amp;nbsp;1981&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Threads and Input/Output in the Synthesis Kernel&amp;#8221;, Massalin and Pa,&amp;nbsp;1989&lt;/li&gt;
&lt;li&gt;&lt;span class=&quot;dquo&quot;&gt;&amp;#8220;&lt;/span&gt;Concurrency Control Performance Modeling: Alternatives and Implications&amp;#8221;, Agrawal, Carey and Livny,&amp;nbsp;1987&lt;/li&gt;
&lt;/ul&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Background&lt;/h3&gt;
&lt;p&gt;I know I said I expected Birrell&amp;#8217;s paper as base knowledge, but here&amp;#8217;s a &lt;span class=&quot;caps&quot;&gt;TLDR&lt;/span&gt; that might let you skip&amp;nbsp;it.&lt;/p&gt;
&lt;p&gt;The need for locking is derived from the concurrent reader/writer problem. It&amp;#8217;s safe for multiple threads to be reading the same data at the same time, but it&amp;#8217;s not safe to read or write while someone else is writing since you can get corrupted results. This requires the idea of the &lt;a href=&quot;http://en.wikipedia.org/wiki/Readers-writer_lock&quot;&gt;reader-writer lock&lt;/a&gt;, which allows any number of concurrent readers, but will make sure that any writer gets exclusive access (i.e. no other readers or writers are accessing the protected data). This is also called shared/exclusive locking, and is an especially common construct in parallel&amp;nbsp;programming.&lt;/p&gt;
&lt;h3&gt;Granularity of&amp;nbsp;Locks&lt;/h3&gt;
&lt;p&gt;The important takeaway from this Jim Gray paper is the idea of &lt;em&gt;hierarchal locking&lt;/em&gt;, where locking a database table also locks all the rows and row fields in that table. This hierarchal structure allows locking at an almost arbitrary granularity depending on the needs of the executing query, which ameliorates the issues that can happen with too-fine-grained locking (excessive lock overhead from doing lots of acquisitions and releases) or too-coarse-grained locking (poor concurrency from unnecessary lock&amp;nbsp;contention). &lt;/p&gt;
&lt;p&gt;This scheme applies to exclusive (X) locks used for writes as well as share (S) locks used for reads, but also requires the introduction of a third lock type: &lt;em&gt;intention locks&lt;/em&gt;. Intention locks are used to indicate in an ancestor node that one of its children has been locked, preventing another query from falsely locking the ancestor, and thus, the child as well. This is refined to having both a &lt;em&gt;intention share lock&lt;/em&gt; (&lt;span class=&quot;caps&quot;&gt;IS&lt;/span&gt;) and an &lt;em&gt;intention exclusive lock&lt;/em&gt; (&lt;span class=&quot;caps&quot;&gt;IX&lt;/span&gt;) to allow concurrent reads, since intention share locks are compatible. Exclusive intention locks are also compatible, since they still have to ultimately exclusively lock the child they want to modify. Queries are required to leave a breadcrumb trail of correct intention locks behind as they traverse toward what they ultimately want to access. Locks also must be released in leaf-to-root order, so the locking hierarchy remains&amp;nbsp;consistent.&lt;/p&gt;
&lt;p&gt;One more intention lock type is introduced for yet better concurrency: &lt;em&gt;share and intention exclusive locks&lt;/em&gt; (&lt;span class=&quot;caps&quot;&gt;SIX&lt;/span&gt;). This is interpreted as &amp;#8220;read-only access to a subtree, exclusively-locking some to be written&amp;#8221;. This is necessary because normally you can&amp;#8217;t have concurrent read/writes (cannot just first acquire the share lock and then an intention exclusive lock since they&amp;#8217;re incompatible), but since these rights are being granted to the same query, it can be depended upon not to read a row that it&amp;#8217;s currently writing. This read-modify-write behavior for a subtree is super common in databases, which is why &lt;span class=&quot;caps&quot;&gt;SIX&lt;/span&gt; locks are&amp;nbsp;important.&lt;/p&gt;
&lt;p&gt;Table 1 on page 5 of the paper is a concise rundown of what locks are compatible with each other. It might be a nice exercise to work through (the lock types being null, &lt;span class=&quot;caps&quot;&gt;IS&lt;/span&gt;, &lt;span class=&quot;caps&quot;&gt;IX&lt;/span&gt;, S, &lt;span class=&quot;caps&quot;&gt;SIX&lt;/span&gt;,&amp;nbsp;X).&lt;/p&gt;
&lt;p&gt;The rest of the paper seems less relevant. Gray et al. explain how this can be applied to a more dynamic graph based on locking ranges of mutable indexes, with the same strategy. I don&amp;#8217;t think this works for multiple indexes, since then the hierarchy &lt;span class=&quot;caps&quot;&gt;DAG&lt;/span&gt; is no longer a &lt;span class=&quot;caps&quot;&gt;DAG&lt;/span&gt;. They also cover the idea of &amp;#8220;degrees of consistency&amp;#8221; with tradeoffs between performance (concurrency) and recovery (consistency). I don&amp;#8217;t think real-world databases use anything except the highest degree of consistency, since the idea of unrecoverable, non-serializable transactions isn&amp;#8217;t pleasant. Anything with eventual consistency (looking at you, NoSQL) has made this&amp;nbsp;tradeoff.&lt;/p&gt;
&lt;h3&gt;Experience with Processes and Monitors in&amp;nbsp;Mesa&lt;/h3&gt;
&lt;p&gt;This paper specifies a new parallel programming language, Mesa, used to implement a new operating system, Pilot. Introducing a new language and &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; at the same time is pretty common, and is how we arrived at C and Unix. This is where the idea of &amp;#8220;Mesa semantics&amp;#8221; for monitors came from (compared to Hoare semantics). To put the work in proper context, apparently in 1979 one had to justify why a preemptive scheduler is required over a non-preemptive design even assuming a uniprocessor (the obvious reason being interrupt&amp;nbsp;handling).&lt;/p&gt;
&lt;p&gt;Mesa is kind of a neat language, in that any procedure can be easily forked off as a new process, and processes are first class values in the language and can be treated like any other value. This isn&amp;#8217;t to say that everything is expected to be able to run concurrently, just that the &lt;code&gt;FORK&lt;/code&gt; language construct is easy to apply. The core organizational construct in Mesa is the &amp;#8220;module&amp;#8221; or the &amp;#8220;monitor module&amp;#8221;. This is basically a way of logically organizing procedures, and specifying which of them need to acquire the monitor lock as part of&amp;nbsp;execution.&lt;/p&gt;
&lt;p&gt;This is also where &amp;#8220;Mesa semantics&amp;#8221; come in. Instead of immediately switching to a waiting process on a signal, the signaller continues running. This seems like a great win, since although it means slightly different semantics to the program, it also means fewer context&amp;nbsp;switches.&lt;/p&gt;
&lt;p&gt;The paper goes on to describe more about monitors and the&amp;nbsp;implementation.&lt;/p&gt;
&lt;h3&gt;On Optimistic Methods for Concurrency&amp;nbsp;Control&lt;/h3&gt;
&lt;p&gt;When I read this paper for 262A, it was a big eye-opener. I felt that the idea wouldn&amp;#8217;t hold up in real usage (and I think that this is true, except in the specific situations noted), but it was a refreshing approach to handling concurrency I had never thought&amp;nbsp;of.&lt;/p&gt;
&lt;p&gt;The idea behind &amp;#8220;optimistic concurrency&amp;#8221; is doing away with locking, and instead doing checking at the end before commit to see if there are any conflicts from concurrent queries, and aborting if so. In this way, even if incorrect query results are generated along the way, they are not externalized. This is speculative and will result in lots of aborted transactions (and thus wasted work) under write-heavy workloads, but as the paper says, this works wonderfully for read-heavy workloads where it&amp;#8217;s unlikely to have a&amp;nbsp;conflict.&lt;/p&gt;
&lt;p&gt;The motivation here is that locking often imposes unnecessary overhead, and can complicate things. In a locking scheme, even read-only queries need to lock rows even though they aren&amp;#8217;t modifying the data, just to indicate that the reads is happening. All this checking and verifying adds up, increasing complexity of the system, and leading to potential deadlocks which have to be resolved through a deadlock-free scheme, or deadlock detection and abort. Locking also can operate at too coarse a granularity; imagine the root node in a hierarchical locking scheme as described by Gray et al., it&amp;#8217;s basically constantly under lock contention, often times unnecessarily since queries are not necessarily operating on the same&amp;nbsp;subtrees.&lt;/p&gt;
&lt;p&gt;This is implemented by having &lt;em&gt;two-phase transactions&lt;/em&gt;, which goes &lt;em&gt;read phase&lt;/em&gt;, &lt;em&gt;validate&lt;/em&gt;, then &lt;em&gt;write phase&lt;/em&gt;. In the read phase, the transaction gathers up the names of all the objects it needs to read, defining a &lt;em&gt;read set&lt;/em&gt;. It then validates whether the transaction T_j is &lt;a href=&quot;http://en.wikipedia.org/wiki/Serializability&quot;&gt;serializable&lt;/a&gt;, checking to make sure that for all prior transactions T_i one of the following is&amp;nbsp;true:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;T_i completes its write phase before T_j starts its read phase. T_i comes entirely before T_j, so it&amp;#8217;s&amp;nbsp;fine.&lt;/li&gt;
&lt;li&gt;The write set of T_i does not intersect with the read set of T_j, and the T_i finishes writing before T_j starts writing. As long as there&amp;#8217;s no intersection, T_j&amp;#8217;s reads are safe, and as long as T_i finishes writing before T_j starts, T_j will not be overwritten by&amp;nbsp;T_i.&lt;/li&gt;
&lt;li&gt;The write set of T_i does not intersect the read or write set of T_j, and T_i finishes reading before T_j starts writing. Similar to the previous, no intersection with the read or write set makes T_j very safe, and T_i needs to finish reading before T_j starts writing to protect T_i&amp;#8217;s read&amp;nbsp;set.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This means that validation needs to check the write sets of all transactions that had finished reading but not finished writing (conditions 2 and 3). This poses an issue for long-running transactions, since the validator might be expected to keep around write-sets almost indefinitely. The proposed answer is to abort and restart the transaction, which leads to the question of how to deal with transactions that repeatedly fail validation. This is answered by write-locking the entire database and letting the &amp;#8220;starving&amp;#8221; transaction run to completion in isolation. This isn&amp;#8217;t great at all, but the assumption is that both long-running transactions and repeat-failures are&amp;nbsp;rare.&lt;/p&gt;
&lt;p&gt;The evaluation in this paper is kind of spotty. It&amp;#8217;s purely theoretical, and they chose to do their analysis on a B-tree, which is one of the better (though also common) situations because of the high fanout and low depth leading to lock contention on root nodes. They also assume a uniformly random distribution for accesses which is probably untrue (accesses are normally temporally correlated, which is why &lt;span class=&quot;caps&quot;&gt;LRU&lt;/span&gt; caching&amp;nbsp;works).&lt;/p&gt;
&lt;p&gt;All-in-all, this is the system design maxim of &amp;#8220;optimize for the common case&amp;#8221; taken to the extreme. The common case of no-conflict transactions will be faster with optimistic concurrency control, but it&amp;#8217;ll collapse under load a lot worse. Like the authors say, it&amp;#8217;s only for situations where transaction conflict is&amp;nbsp;rare.&lt;/p&gt;
&lt;h3&gt;Threads and Input/Output in the Synthesis&amp;nbsp;Kernel&lt;/h3&gt;
&lt;p&gt;This seems to be a more meta paper, where optimistic concurrency and lock-avoiding techniques were applied to an &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; to improve performance. It&amp;#8217;s also chock-full of system-specific jargon, which I will kindly avoid introducing. Honestly, most of what&amp;#8217;s laid out in the paper feels like a bunch of small optimizations for a specialized kernel that add up to something that performs demonstrable better than SunOS. Some of these techniques might be translatable back to Unix-y implementations, some of it is unique to the system (runtime optimization of syscalls?), and some of it is because it&amp;#8217;s a special-purpose kernel. I like how the references are to the SunOS source code, &lt;span class=&quot;caps&quot;&gt;GEB&lt;/span&gt; (yes, the Hofstadter book), 3 of the author&amp;#8217;s own papers, and then two external. Certainly a different&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;The takeaways here are unclear. The conclusion of &amp;#8220;avoid synchronization when possible&amp;#8221; seems hardly novel, and it feels too much like they implemented to optimize their microbenchmarks (no real apps were&amp;nbsp;written).&lt;/p&gt;
&lt;h3&gt;Concurrency Control Performance&amp;nbsp;Modeling&lt;/h3&gt;
&lt;p&gt;This paper does a deep comparison between three concurrency control algorithms: blocking locks, immediate restart on lock contention, and optimistic concurrency. I really love papers like this one, since they take a bunch of different algorithms that all tested well under different model assumptions, carefully dismantle said assumptions, and reveal real truths with their own meticulous performance model. It really demonstrates the authors&amp;#8217; complete understanding of the problem at&amp;nbsp;hand.&lt;/p&gt;
&lt;p&gt;There are a number of model parameters that are crucial to performance here. The &lt;em&gt;database system model&lt;/em&gt; specifies the physical hardware (CPUs and disks), associated schedulers, characteristics of the &lt;span class=&quot;caps&quot;&gt;DB&lt;/span&gt; (size and granularity), load control mechanisms, and the concurrency control algorithm itself. The &lt;em&gt;user model&lt;/em&gt; specifies the arrival process for users, and the type of transactions (batch or interactive). The &lt;em&gt;transaction model&lt;/em&gt; specifies the storage access pattern of a transaction, as well as its computational requirements (expressed in terms of&amp;nbsp;probabilities). &lt;/p&gt;
&lt;p&gt;I consider this to be about as complete as possible. They ignore I/O patterns and cache behavior, but those are just damn hard to model. Using a Poisson distribution for transaction inter-arrival rates is canonical without evidence to disprove it ( see &amp;#8220;On the Self-similar Nature of Ethernet Traffic&amp;#8221; by Leland et al. for a situation where Poisson does not hold so true). They also do not take into account processing time spent on the concurrency control algo itself, which feels like a slight copout since I think this means they ignore lock overhead and use a completely granular locking system (not hierarchical locking), which disfavors optimistic concurrency. This is implementation specific and a lot of additional work to add to the model, and considering there&amp;#8217;s some prior work showing that the costs are roughly comparable and negligible compared to access costs, I&amp;#8217;m willing to let it&amp;nbsp;go.&lt;/p&gt;
&lt;p&gt;The interesting part comes when they manipulate all the model parameters, and explain how different papers arrived at their different performance results. Basically, under the assumption of infinite resources, optimistic concurrency does splendidly as the level of multiprogramming increases, since locking strategies run into contention and transactions get blocked up. Optimistic transactions still face more conflicts and have to be restarted, but since there the pipeline is always full of running transactions (none are just blocked and using up a queue slot while doing no work), overall throughput continues to increase. Immediate-restart reaches an interesting performance plateau, due to its scheme of trying to match the rate of transaction completion with the rate of re-introducing restarted transactions. This was the model used in a number of prior&amp;nbsp;papers.&lt;/p&gt;
&lt;p&gt;Introducing a very resource limited situation turns things sharply in favor of blocking algos. Blocking performs much better until very high levels of multiprogramming, immediate-restart hits the same plateau for the same reason, and optimistic concurrency performs linearly worse beyond a very small multiprogramming level. Basically, every optimistic conflict detected at the end of a transaction just wasted all of the resources used; immediate restart does better since it will restart if it detects a conflict midway, and also delays restarts to match the completion&amp;nbsp;rate.&lt;/p&gt;
&lt;p&gt;Increasing the number of resources begins to favor optimistic concurrency again, but the price/performance isn&amp;#8217;t there since doubling the # of resources does not lead to a doubling in performance. They do a few more different situations, examining different workloads and model assumptions, which you can read yourself if you want to know&amp;nbsp;more.&lt;/p&gt;
&lt;p&gt;Basically, it&amp;#8217;s hard to make general statements about performance; things are dependent on your model. It seems that for most real-world use cases though (limited resources, high utilization), blocking is the concurrency control method of choice. It&amp;#8217;s also important to carefully control the level of multiprogramming for optimal throughput, since performance tends to peak and then decline as things&amp;nbsp;thrash.&lt;/p&gt;
&lt;p&gt;I also just find it really cool that they explained the performance results of a lot of previous papers within the scope of their own model, basically saying that no one was wrong, just incomplete in their&amp;nbsp;analysis.&lt;/p&gt;

   </content></entry><entry><title>Virtual memory review</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/virtual_memory.html"/><updated>2011-06-11T18:33:00Z</updated><published>2011-06-11T18:33:00Z</published><id>http://www.umbrant.com/blog/2011/virtual_memory.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;I&amp;#8217;m taking the &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; prelim this fall, which means I have to read ~100 papers this summer for background material. Since repetition aids retention, I&amp;#8217;m putting notes for papers I read up on my blog. The topics are wide-ranging, so I&amp;#8217;m trying to start with the fundamentals and then move on up to the whole-system&amp;nbsp;papers.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m kicking it off with &amp;#8220;Virtual Memory, Processes, and Sharing in &lt;span class=&quot;caps&quot;&gt;MULTICS&lt;/span&gt;&amp;#8221; by Daley and Dennis (1968) and &amp;#8220;The Multics Virtual Memory: Concepts and Design&amp;#8221; by Bensoussan, Clingen, and Daley (1972). Learn about the joys of segmentation and dynamic linking from classic papers from the 70s! These are slightly infamous papers for some systems students here at Berkeley, due to a certain past &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; prelim examiner grilling them on exactly these&amp;nbsp;details.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Background&amp;nbsp;Review&lt;/h3&gt;
&lt;p&gt;Some people consider &lt;a href=&quot;http://en.wikipedia.org/wiki/Memory_segmentation&quot;&gt;segmentation&lt;/a&gt; to be the most natural way of structuring a program. Most programs are basically a collection of libraries (something really true in modern software engineering). In a segmented virtual memory system, each distinct library is placed in its own separate segment such that it has its own address space. They&amp;#8217;re still all mapped unto the same flat physical memory address space, but through per-segment base and offset&amp;nbsp;addresses.&lt;/p&gt;
&lt;p&gt;This isn&amp;#8217;t actually used much in modern operating systems for reasons I&amp;#8217;m not entirely aware of (I&amp;#8217;d guess simplicity and performance), but it&amp;#8217;s a pleasingly abstract and indirect way of organizing a program (a hallmark of&amp;nbsp;Multics).&lt;/p&gt;
&lt;h3&gt;Virtual Memory, Processes, and Sharing in&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;MULTICS&lt;/span&gt;&lt;/h3&gt;
&lt;p&gt;Multics structured its programs in terms of segments, which could be read/write/execute protected. Segmentation is great for doing memory protection (and something that only recently reemerged as the &lt;a href=&quot;http://en.wikipedia.org/wiki/NX_bit&quot;&gt;&lt;span class=&quot;caps&quot;&gt;NX&lt;/span&gt; bit&lt;/a&gt; for flat memory models), since it&amp;#8217;s easy to do a compare on any &lt;code&gt;(base+offset)&lt;/code&gt; calculation and see if it falls into a protected range. Segments can still of course be paged, and segmentation and paging are complementary: segmentation for protection, and paging for working set&amp;nbsp;management.&lt;/p&gt;
&lt;p&gt;Addressing in Multics is done in terms of a &lt;em&gt;generalized address&lt;/em&gt;: &lt;code&gt;(segment num + word num)&lt;/code&gt;. The segment number of the currently executing segment is stored in the &lt;em&gt;procedure base register&lt;/em&gt;, so most instructions just need to specify a word number. Indirect addressing (i.e. referencing an address stored at an address) is done with a pair of instructions to have enough bits: one for the segment number, one for the word&amp;nbsp;number.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;descriptor table&lt;/em&gt; is kept of all the segments in a program to map them to hardware addresses. This table maps seg nums to a physical address, and then adds the word num as the offset. This is similar to a page table: virtual to physical address translation. A pointer to the descriptor table is saved as part of the context information of the&amp;nbsp;process.&lt;/p&gt;
&lt;p&gt;Now for the complicated bits: dynamic linking. Clearly, we don&amp;#8217;t want each program to have to have its own copy of shared segments (say, libc), and we want some abstraction so we aren&amp;#8217;t hardcoding word numbers into our program. This also needs to work for segments linking to other segments which link to other segments, etc., so it gets a little hairy. We also want this to be reasonably fast, e.g. don&amp;#8217;t do multiple memory accesses for every dynamically linked call, at least after the first time. In list&amp;nbsp;form:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Dynamically linked accesses are specially marked in the program&amp;nbsp;text&lt;/li&gt;
&lt;li&gt;Dynamically linked segments are present at a well-know &lt;em&gt;path name&lt;/em&gt;, e.g. in Linux &lt;code&gt;/usr/lib/ld-linux.so.2&lt;/code&gt;, that the system can search for and find (see&amp;nbsp;LD_LIBRARY_PATH).&lt;/li&gt;
&lt;li&gt;Each segments presents a &lt;em&gt;symbol table&lt;/em&gt;, which defines the call-in points for the segment (static vars, functions,&amp;nbsp;etc)&lt;/li&gt;
&lt;li&gt;Initially, all calls to an external function are to an indirect reference stored as &lt;em&gt;link data&lt;/em&gt; in a per-segment &lt;em&gt;linkage table&lt;/em&gt;. This table is initially set to trap to an &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; lookup&amp;nbsp;function. &lt;/li&gt;
&lt;li&gt;A &lt;em&gt;linkage pointer&lt;/em&gt; to the &lt;em&gt;linkage table&lt;/em&gt; is maintained to switch around the table as context changes to different&amp;nbsp;segments.&lt;/li&gt;
&lt;li&gt;On the first reference, the &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; lookup function finds the file of the external segment, examines its symbol table, and then &lt;em&gt;links&lt;/em&gt; the two segments by updating the link data in the linkage table. Future references use that indirect address to go straight to the external&amp;nbsp;segment.&lt;/li&gt;
&lt;li&gt;A further complication enters when switching the linkage pointers between linked segments. To determine the new value for the linkage pointer, the calling procedure actually calls into the new segment&amp;#8217;s linkage table, which has special instructions to fixup the linkage pointer and then call the called procedure.&lt;ul&gt;
&lt;li&gt;Thus: Caller -&amp;gt; Caller&amp;#8217;s linkage table ~&amp;gt; Callee&amp;#8217;s linkage table ~&amp;gt;&amp;nbsp;Callee&lt;/li&gt;
&lt;li&gt;This is direct -&amp;gt; indirect -&amp;gt; indirect, plus a fixup, seems&amp;nbsp;expensive&amp;#8230;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This really isn&amp;#8217;t that different from how Linux does it, the basic idea of &amp;#8220;keep a well-known table that fixes itself on the first reference&amp;#8221; is a winner. I feel like there are a lot of memory accesses required to traverse all these layers of indirection, since you are calling through multiple layers of indirect addressing, each of which is a pair of&amp;nbsp;instructions.&lt;/p&gt;
&lt;h3&gt;The Multics Virtual Memory: Concepts and&amp;nbsp;Design&lt;/h3&gt;
&lt;p&gt;It&amp;#8217;s weird to hear that back in the days of yore, files could not be easily loaded as program text, and the idea of virtual memory for protection, abstraction, and programmer benefit was a new idea. Users were just allocated a range of memory, or &lt;em&gt;core image&lt;/em&gt;, with no sharing between users; if you wanted to work on a &amp;#8220;shared file&amp;#8221;, that meant doing I/O to copy it into your range, and then another I/O to put it back into the filesystem. Each user&amp;#8217;s core image was also an unstructured jumble of instructions and data, which makes system-level sharing and memory protection basically&amp;nbsp;impossible.&lt;/p&gt;
&lt;p&gt;These two goals motivated the design of virtual memory in Multics: sharing and protection. This led to the &lt;em&gt;segmentation&lt;/em&gt;, where each segment appears as a flat, linear namespace to the user program, with read/write/execute/append access rights attached as metadata to the&amp;nbsp;segment.&lt;/p&gt;
&lt;p&gt;Segments are also paged to ease the allocation problem and to support large segments. &lt;em&gt;Descriptor segments&lt;/em&gt; (aka descriptor tables) are also paged for good reason, meaning 4 memory lookups to access a memory location, going through two page tables (one for the descriptor, one for the segment). TLBs work here, but it still sounds slow. Page tables are also a static size, not a&amp;nbsp;tree.&lt;/p&gt;
&lt;p&gt;This paper is a decent overview of Multics, probably would have made sense to read it before the dynamic linking&amp;nbsp;one.&lt;/p&gt;

   </content></entry><entry><title>Android: the Good, the Bad, and the Ugly</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/android_the_good_and_bad.html"/><updated>2011-05-15T23:54:00Z</updated><published>2011-05-15T23:54:00Z</published><id>http://www.umbrant.com/blog/2011/android_the_good_and_bad.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;Over the last week, I&amp;#8217;ve been doing a crash course in Android programming. &lt;a href=&quot;http://www.eecs.berkeley.edu/~rxin/&quot;&gt;Reynold&lt;/a&gt; and I have been working on our combination &lt;span class=&quot;caps&quot;&gt;CS294&lt;/span&gt;-35 Mobile Development project and &lt;span class=&quot;caps&quot;&gt;AMP&lt;/span&gt; Lab retreat demo, which is next week. Coming from a traditional web and application development background, there are some things Android does comparatively well, some comparatively poorly, and some that just irritate&amp;nbsp;me.&lt;/p&gt;
&lt;p&gt;This isn&amp;#8217;t a tutorial, but it&amp;#8217;d probably be a useful read if you&amp;#8217;re thinking of getting started with Android development. My pain, your&amp;nbsp;gain.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;The&amp;nbsp;Good&lt;/h3&gt;
&lt;p&gt;I like the relatively smooth integration of the Android &lt;span class=&quot;caps&quot;&gt;SDK&lt;/span&gt; with Eclipse. It&amp;#8217;s pretty easy getting to the &lt;a href=&quot;http://developer.android.com/guide/tutorials/hello-world.html&quot;&gt;Hello World&lt;/a&gt; stage with the Android emulator. Autocompletion works as expected. Debugging is easy, since the emulator will throw stacktraces in LogCat even when not in debug mode, and it&amp;#8217;s just as easy to do these things on actual phone hardware. It was also easy to get things running on a real phone; the Motorola Droid I was using didn&amp;#8217;t require Verizon activation or rooting or paying for a developers license. Check a few boxes in the settings menu, and you&amp;#8217;re off to the&amp;nbsp;races.&lt;/p&gt;
&lt;p&gt;There&amp;#8217;s also a thriving community of Android developers. It&amp;#8217;s very easy to Google your problems (and believe me, that&amp;#8217;s important). StackOverflow seems very&amp;nbsp;helpful.&lt;/p&gt;
&lt;p&gt;I also appreciate having an emulator that works so effectively. I wish that the emulator&amp;#8217;s camera had a more useful test pattern, but that&amp;#8217;s forgivable. Otherwise, I had very few situations where the emulator behavior differed from actual&amp;nbsp;hardware.&lt;/p&gt;
&lt;h3&gt;The&amp;nbsp;Bad&lt;/h3&gt;
&lt;p&gt;There&amp;#8217;s a pretty large jump between Hello World and the next, less trivial tutorial in the series (&lt;a href=&quot;http://developer.android.com/guide/tutorials/notepad/index.html&quot;&gt;Notepad&lt;/a&gt;), and there&amp;#8217;s a huge jump from Notepad to making your own&amp;nbsp;app.&lt;/p&gt;
&lt;p&gt;Notepad introduces what has to be the messiest part of Android (and indeed, mobile) development: the &lt;a href=&quot;http://developer.android.com/reference/android/app/Activity.html&quot;&gt;application lifecycle&lt;/a&gt;. Take a look at the flowchart in that link. An Activity is basically a single screen of an application. Whene a new activity is switched in, the old one goes through &lt;code&gt;onPause()&lt;/code&gt; and maybe &lt;code&gt;onStop()&lt;/code&gt;. After that, it&amp;#8217;s fair game for the Android task killer, which starts discriminately killing off applications if memory is&amp;nbsp;low.&lt;/p&gt;
&lt;p&gt;This is a huge hassle from a traditional app developer standpoint, since it used to be that the &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; would save all your application state on a context switch, and restore it when your app is switched back in. Modify some variables, context switch out, context switch back in, and the variables are how you left them. In Android, that&amp;#8217;s no longer true. Now, you are forced to serialize all (all!) of your live state out to one of the Android &lt;a href=&quot;http://developer.android.com/guide/topics/data/data-storage.html&quot;&gt;persistent datastores&lt;/a&gt;, which most of the time means using (SQLite)[http://developer.android.com/guide/topics/data/data-storage.html#db]. If you don&amp;#8217;t do this, it means that your app works correctly most of the time (since the task killer doesn&amp;#8217;t always kick in), but occasionally, bad things will happen: settings get reset, entered form data disappears, just plain&amp;nbsp;bugs.&lt;/p&gt;
&lt;p&gt;This becomes especially terrible when you&amp;#8217;re doing any kind of network programming. This introduces a whole mess of concepts that&amp;#8217;d require another full blog post (&lt;a href=&quot;http://developer.android.com/reference/android/app/Service.html&quot;&gt;Service&lt;/a&gt;, &lt;a href=&quot;http://developer.android.com/reference/android/content/ContentProvider.html&quot;&gt;ContentProvider&lt;/a&gt;, and &lt;a href=&quot;http://developer.android.com/guide/topics/fundamentals/processes-and-threads.html&quot;&gt;background reading&lt;/a&gt;), but the basic problem is, how do you reliably make a request to a server if your request might die at any time (due to the application&amp;nbsp;lifecycle)?&lt;/p&gt;
&lt;p&gt;You end up having to watch this &lt;a href=&quot;http://www.youtube.com/watch?v=xHXn3Kg2IQE&quot;&gt;Google I/O talk on how to correctly implement &lt;span class=&quot;caps&quot;&gt;REST&lt;/span&gt; in Android&lt;/a&gt;, which features this wonderful diagram that Reynold and I&amp;nbsp;implemented:&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;ContentProvider for REST&quot; src=&quot;android_the_good_the_bad_and_the_ugly/android_rest_diagram.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;Yuck. Wasn&amp;#8217;t &lt;span class=&quot;caps&quot;&gt;REST&lt;/span&gt; supposed to be the easy&amp;nbsp;way?&lt;/p&gt;
&lt;p&gt;The very naive way of doing network requests is directly in your Activity, which blocks the &lt;span class=&quot;caps&quot;&gt;UI&lt;/span&gt; thread (leading to the app freezing) and is clearly bad practice, even to a novice. The slightly-less-naive way is using a Service, in which you have to start a new thread to make the request (or else it will again block the &lt;span class=&quot;caps&quot;&gt;UI&lt;/span&gt; thread). This will mostly work, since Services sort of run in the background, but is still prone to erratic bugs because Services are still under the authority of the Android task killer. So, you end up resorting to ContentProvider, and the filling out the 6 boxes in the diagram you see&amp;nbsp;above.&lt;/p&gt;
&lt;p&gt;The same story can be found for getting phone location. The &lt;a href=&quot;http://developer.android.com/guide/topics/location/obtaining-user-location.html&quot;&gt;documentation page&lt;/a&gt; isn&amp;#8217;t bad, but it quickly becomes obvious that it&amp;#8217;s complicated to do it both correctly and well. Your app has to balance using &lt;span class=&quot;caps&quot;&gt;GPS&lt;/span&gt; vs. celltower triangulation based on accuracy and availability, cache old locations to get an initial fast fix, invalidate said cached locations if they&amp;#8217;re too old or too inaccurate, and minimizing overall usage of these radios since they&amp;#8217;re the biggest battery killers in a phone. It&amp;#8217;s a lot of manual heavy lifting to do it right, and it&amp;#8217;s easy to do it incorrectly (presenting inaccurate result) and poorly (quickly draining&amp;nbsp;battery).&lt;/p&gt;
&lt;h3&gt;The&amp;nbsp;Ugly&lt;/h3&gt;
&lt;p&gt;These are some random warts in the platform, not fundamental issues, but annoying (and&amp;nbsp;fixable).&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There are a lot of concepts thrown at you in the tutorials, and Notepad doesn&amp;#8217;t go far enough. I&amp;#8217;d like to see some more tutorials, and more beginner-friendly explanations of classes like Activity, Service, Intent, and View, and some high-level advice on designing for the antagonistic application&amp;nbsp;lifecycle.&lt;/li&gt;
&lt;li&gt;The development emulator is really slow. It takes a few minutes to start up, and there&amp;#8217;s definitely lag while operating it. Comparatively, the iPhone emulator starts almost instantaneously, and feels just as snappy as a real&amp;nbsp;iPhone.&lt;/li&gt;
&lt;li&gt;No tutorial on how to programatically build a &lt;span class=&quot;caps&quot;&gt;UI&lt;/span&gt;. I get that &lt;span class=&quot;caps&quot;&gt;XML&lt;/span&gt; is the preferred way since you can do it graphically in Eclipse, but that falls short pretty&amp;nbsp;fast.&lt;/li&gt;
&lt;li&gt;The &lt;span class=&quot;caps&quot;&gt;XML&lt;/span&gt; layout is decidedly less powerful than &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt;+&lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt;. No templates, no style rules, lots of repeating the same padding, margin, and textsize parameters in each file. It&amp;#8217;s a nightmare for maintainability; fortunately mobile UIs are&amp;nbsp;simple.&lt;/li&gt;
&lt;li&gt;There isn&amp;#8217;t a provided library of icons. I think this is a no brainer; I want some basic icons for things like &amp;#8220;list&amp;#8221;, &amp;#8220;settings&amp;#8221;, &amp;#8220;home&amp;#8221; that I see used in core Android, but these aren&amp;#8217;t available in the&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;SDK&lt;/span&gt;.&lt;/li&gt;
&lt;li&gt;I can&amp;#8217;t figure out how to do not-fullscreen Google Maps with panning and zoom. iPhone can do it, but somehow all the Android maps I see that support pan+zoom are&amp;nbsp;fullscreen.&lt;/li&gt;
&lt;li&gt;Hardware and software keyboards have different event listeners and behaviors. Software keyboard has an unfortunate habit of staying open even when changing tabs in a&amp;nbsp;TabView.&lt;/li&gt;
&lt;li&gt;I disliked having to write something like 4 serialization/deserialization routines for every object. This was due to having to store all my state as Java objects, in SQLite, in &lt;span class=&quot;caps&quot;&gt;JSON&lt;/span&gt; to talk to the server over &lt;span class=&quot;caps&quot;&gt;REST&lt;/span&gt;, and also as visible data on screen. An &lt;span class=&quot;caps&quot;&gt;ORM&lt;/span&gt; or something would be&amp;nbsp;great.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;This experience made me realize why many Android apps suck: it&amp;#8217;s hard to do things the right way, and easy to hack it together the wrong way. I don&amp;#8217;t think Android is alone on this one, from what I hear, iPhone isn&amp;#8217;t much different in terms of application lifecycle. From the &lt;span class=&quot;caps&quot;&gt;OS&lt;/span&gt; point of view, it&amp;#8217;s great that any app can be killed at any time to save resources. I&amp;#8217;m betting this results in huge wins in battery life, performance, and code size. However, it just shifts that burden onto app developers, who aren&amp;#8217;t used to doing this kind of thing, and there aren&amp;#8217;t libraries or APIs in place to make this as easy as it should&amp;nbsp;be.&lt;/p&gt;
&lt;p&gt;I&amp;#8217;m not totally turned off of mobile app development, since I still believe that mobile is essentially the future of computing, but I really think it could be a lot better. Personally, &lt;span class=&quot;caps&quot;&gt;HP&lt;/span&gt;&amp;#8217;s webOS appeals to me since &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt;+&lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt;+&lt;span class=&quot;caps&quot;&gt;JS&lt;/span&gt; is a &lt;em&gt;much&lt;/em&gt; more natural way of writing applications (and that&amp;#8217;s not just my bias as a web developer), and more pure Linux-based OSs like MeeGo are certainly easier to program (but then you lose the noted benefits of the &amp;#8220;kill anything at anytime&amp;#8221; model). I&amp;#8217;m still willing to bet on Android, but it still needs a lot of work before it&amp;#8217;s a first-class application development&amp;nbsp;environment.&lt;/p&gt;

   </content></entry><entry><title>External sorting of large datasets</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/external_sorting.html"/><updated>2011-04-16T17:24:00Z</updated><published>2011-04-16T17:24:00Z</published><id>http://www.umbrant.com/blog/2011/external_sorting.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;This is a common interview question: how do you sort data that is bigger than memory? &amp;#8220;Big data&amp;#8221; in the range of tera or petabytes can now almost be considered the norm (think of Google saving every search, click, and ad impression ever), so this manifests in reality as well. This is also a canonical problem in the database world, where it is referred to as an &amp;#8220;external&amp;nbsp;sort&amp;#8221;.&lt;/p&gt;
&lt;p&gt;Your mind should immediately turn to divide and conquer algorithms, namely merge sort. Write out intermediate merged output to disk, and read it back in lazily for the next round. I decided this would be a fun implementation and optimization exercise to do in C. There will probably be a follow-up post, since there are lots of optimizations I haven&amp;#8217;t yet&amp;nbsp;implemented.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;h3&gt;Introduction&lt;/h3&gt;
&lt;p&gt;Guido van Rossum (the creator of Python) did this a while ago for the rather smaller (and simpler) case of &lt;a href=&quot;http://neopythonic.blogspot.com/2008/10/sorting-million-32-bit-integers-in-2mb.html&quot;&gt;sorting a million 32-bit integers in &lt;span class=&quot;caps&quot;&gt;2MB&lt;/span&gt; of &lt;span class=&quot;caps&quot;&gt;RAM&lt;/span&gt;&lt;/a&gt;. I took the same approach of a merge sort that writes intermediate runs out to files on disk, buffering file I/O to improve performance. However, since I&amp;#8217;m targeting file sizes that are actually larger than &lt;span class=&quot;caps&quot;&gt;RAM&lt;/span&gt; (e.g. a couple gigabytes), I need to do more complicated&amp;nbsp;things.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&quot;http://en.wikipedia.org/wiki/Merge_sort&quot;&gt;basic merge sort&lt;/a&gt; you learn in &lt;span class=&quot;caps&quot;&gt;CS&lt;/span&gt; 101 recurses down to the base case of runs of just 1 element, which are progressively merged together in pairs in a logarithmic fashion (arriving at the ultimate O(n*log n) time complexity). This is inefficient for large datasets, because the merging rate is too low. If you&amp;#8217;re sorting a &lt;span class=&quot;caps&quot;&gt;1GB&lt;/span&gt; file of 32-bit integers, the first round of merging would generate &lt;code&gt;(1GB/sizeof(int)/2) = (2^30/2^2/2) = 2^27&lt;/code&gt; 8-byte files, which is just too many files. This also leads to the second core problem: small disk I/Os are highly inefficient, since they result in expensive disk seeks. Writing a bunch of 8-byte (or even 8-kilobyte) files effectively randomizes your access pattern, and will choke your throughput. To avoid bad seeks, reads and writes need to be done at about the size of the disk&amp;#8217;s buffer (about &lt;span class=&quot;caps&quot;&gt;16MB&lt;/span&gt; these&amp;nbsp;days).&lt;/p&gt;
&lt;p&gt;All of my code is also &lt;a href=&quot;https://github.com/umbrant/extsort&quot;&gt;available on github&lt;/a&gt; if you want to follow along, this post is based more-or-less on the &lt;a href=&quot;https://github.com/umbrant/extsort/tree/3ce53516063bff05570736c412eed032b803ea15&quot;&gt;initial commit&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Basic&amp;nbsp;Approach&lt;/h3&gt;
&lt;p&gt;So our goal is to reduce the number of files written by the first merge step, and also write these files in much bigger chunks. This can be accomplished by increasing the quantum for merging, and doing n-way instead of 2-way&amp;nbsp;merging.&lt;/p&gt;
&lt;p&gt;I increased the merge quantum by sorting each page (&lt;span class=&quot;caps&quot;&gt;4KB&lt;/span&gt;) of initial input with quicksort. This way, even with just 2-way merging, the first round for our &lt;span class=&quot;caps&quot;&gt;1GB&lt;/span&gt; of integers only generates &lt;code&gt;(1GB/page_size/2) = (2^30/2^12/2)&lt;/code&gt; = 2^18 intermediate files, which is a lot better than 2^27, but still too large (a quarter million files is a&amp;nbsp;lot). &lt;/p&gt;
&lt;p&gt;N-way merging merges more (many more) than two runs together at once, and is basically the same algorithm as 2-way merging. This finally reduces the level of fan out to manageable levels, and means that the size of the output runs is much larger, meaning that disk I/O can be more easily batched into large &lt;span class=&quot;caps&quot;&gt;16MB&lt;/span&gt; chunks. With 64-way merging we finally get down to &lt;code&gt;(2^18/2^6) = 2^12&lt;/code&gt;, or 4096 intermediate files, which is a pleasant&amp;nbsp;number.&lt;/p&gt;
&lt;p&gt;A further necessary improvement is to incrementally pull large runs off disk (required for later merge steps, when the runs are too large to all fit into memory). I do this at the same granularity as my other I/O operations: &lt;span class=&quot;caps&quot;&gt;16MB&lt;/span&gt;. Currently, this decides the degree of fan out as well, since I pack as many &lt;span class=&quot;caps&quot;&gt;16MB&lt;/span&gt; buffers into memory as I&amp;#8217;m allowed, and n-way merge across all of them. This could be a problem if oodles and oodles of memory are allocated to the sort (since n gets large), but my computer with &lt;span class=&quot;caps&quot;&gt;4GB&lt;/span&gt; of &lt;span class=&quot;caps&quot;&gt;RAM&lt;/span&gt; can only hold 256 runs, which isn&amp;#8217;t that&amp;nbsp;many.&lt;/p&gt;
&lt;h3&gt;Miscellaneous&amp;nbsp;notes&lt;/h3&gt;
&lt;p&gt;There are a few other miscellaneous notes. I ran into the per-process fd limit when doing large merges, so files have to be closed and reopened at the correct offset. I also parallelize the initial quicksorting of pages with a simple worker pool, which really helps speed up the first layer of merging. 
My quicksort also reduces recursion depth by bubblesorting for runs smaller than 5, which is okay since bubblesort is efficient on tiny sets (worst case 6 compares, 6 swaps, compare that to insertion sort). This might or might not increase performance, but it&amp;#8217;s fun. Finally, even if 256 buffers can fit into memory, one buffer must always be reserved to be an output buffer (meaning you can do at most a 255-way merge). There&amp;#8217;s also some &lt;code&gt;O(n)&lt;/code&gt; memory overhead outside of just storing the data buffers, which you need to be aware of if your memory bound is especially&amp;nbsp;tight.&lt;/p&gt;
&lt;h3&gt;Benchmarking&lt;/h3&gt;
&lt;p&gt;Enough discussion, onto the numbers! This is a situation where I feel like building an autotuner, since my envisioned final version will have a number of knobs to tweak (a future project I suppose). Right now, the two knobs I have to play with are the size of the overall buffer, and the size of I/O&amp;nbsp;buffers. &lt;/p&gt;
&lt;p&gt;I took two sets of numbers. The first set was taken on my laptop, which is a Intel Core i7-620M supporting 4 hyperthreads, &lt;span class=&quot;caps&quot;&gt;4GB&lt;/span&gt; of &lt;span class=&quot;caps&quot;&gt;RAM&lt;/span&gt;, and a 7200 &lt;span class=&quot;caps&quot;&gt;RPM&lt;/span&gt; disk. The second set was taken on my desktop, an &lt;span class=&quot;caps&quot;&gt;AMD&lt;/span&gt; Phenom &lt;span class=&quot;caps&quot;&gt;II&lt;/span&gt; X4 965 Black Edition supporting 4 hardware threads, &lt;span class=&quot;caps&quot;&gt;4GB&lt;/span&gt; of &lt;span class=&quot;caps&quot;&gt;RAM&lt;/span&gt;, and an &lt;span class=&quot;caps&quot;&gt;60GB&lt;/span&gt; &lt;span class=&quot;caps&quot;&gt;OCZ&lt;/span&gt; Vertex 2 &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;. The &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; should help for the smaller I/O buffer sizes, but sequential access shouldn&amp;#8217;t be too far&amp;nbsp;apart.&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Desktop mergesort&quot; src=&quot;external_sorting/extsort_desktop.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;&lt;img alt=&quot;Laptop mergesort&quot; src=&quot;external_sorting/extsort_laptop.png&quot; /&gt;&lt;/p&gt;
&lt;p&gt;I found these numbers pretty interesting. Each line represents a different total memory size. The graphs indicate that increasing the number of I/O buffer pages leads to better performance as expected, but the small total memory sizes end up performing generally better. Furthermore, my laptop performs better than my desktop with the&amp;nbsp;&lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;This can be interpreted as follows. First, linking the fan out of the merge to total memory size is a bad idea. The following table helps make this&amp;nbsp;clear.&lt;/p&gt;
&lt;table border=1&gt;
    &lt;tr&gt;
        &lt;th colspan=4&gt;Fan out of n-way merge&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;&lt;/td&gt;
        &lt;th colspan=3&gt;Number of I/O buffer pages (4k)&lt;/th&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;th&gt;Total memory (&lt;span class=&quot;caps&quot;&gt;MB&lt;/span&gt;)&lt;/th&gt;
        &lt;td&gt;1024&lt;/td&gt;&lt;td&gt;2048&lt;/td&gt;&lt;td&gt;4096&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;64&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;&lt;td&gt;3&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;128&lt;/td&gt;&lt;td&gt;31&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;&lt;td&gt;7&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;256&lt;/td&gt;&lt;td&gt;63&lt;/td&gt;&lt;td&gt;31&lt;/td&gt;&lt;td&gt;15&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;384&lt;/td&gt;&lt;td&gt;95&lt;/td&gt;&lt;td&gt;47&lt;/td&gt;&lt;td&gt;23&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
        &lt;td&gt;512&lt;/td&gt;&lt;td&gt;127&lt;/td&gt;&lt;td&gt;63&lt;/td&gt;&lt;td&gt;31&lt;/td&gt;
    &lt;/tr&gt;
&lt;/table&gt;

&lt;p&gt;By looking at the laptop graph and this table together, we see that high fanout for &lt;span class=&quot;caps&quot;&gt;512MB&lt;/span&gt; is killing performance, since it&amp;#8217;s fine when fan out drops down to 31 at 4096 buffer pages. Conversely, the &lt;span class=&quot;caps&quot;&gt;64MB&lt;/span&gt; case suffers the opposite problem at 4096 pages; a fan out of 3 is too low. Since the two fastest completion times were both with a fan out of 7 (&lt;span class=&quot;caps&quot;&gt;64MB&lt;/span&gt; with 2048 pages, &lt;span class=&quot;caps&quot;&gt;128MB&lt;/span&gt; with 4096 pages), I&amp;#8217;m betting that it&amp;#8217;s around here, but this requires further tuning to decide for&amp;nbsp;sure.&lt;/p&gt;
&lt;p&gt;The second finding is that the sort is currently &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; bound. This isn&amp;#8217;t what I expected since there&amp;#8217;s a lot of disk I/O, but it seems that the I/O batching techniques are effective. Otherwise, the desktop with the &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; should outpace the laptop. Furthermore, since merging is still single-threaded, the i7 laptop actually might have an advantage because of &lt;a href=&quot;http://en.wikipedia.org/wiki/Intel_Turbo_Boost&quot;&gt;Turbo Boost&lt;/a&gt; kicking up single core performance above the Phenom &lt;span class=&quot;caps&quot;&gt;II&lt;/span&gt;&amp;nbsp;desktop.&lt;/p&gt;
&lt;p&gt;Also note that for the relatively low fan outs at 64 and &lt;span class=&quot;caps&quot;&gt;128MB&lt;/span&gt;, the desktop with the &lt;span class=&quot;caps&quot;&gt;SSD&lt;/span&gt; has very flat performance as the size of the I/O buffer changes. This is the beauty of fast random accesses, and might be exploitable for better performance since you can save on memory usage by shrinking the I/O&amp;nbsp;buffers.&lt;/p&gt;
&lt;h3&gt;Future&amp;nbsp;work&lt;/h3&gt;
&lt;p&gt;Both of the aforementioned performance issues can be solved by parallelizing the merge step by running multiple n-way merges simultaneously. This lowers the fanout while still using all available memory, and will better balance &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; and I/O time. The number of threads and fan-out of the merge can be parameterized separately, adding two more tuning knobs to the existing knobs of total memory usage and size of I/O buffer (autotuner&amp;nbsp;time?).&lt;/p&gt;
&lt;p&gt;Another potential performance improvement is &lt;a href=&quot;http://en.wikipedia.org/wiki/Multiple_buffering&quot;&gt;double buffering&lt;/a&gt;. This is essentially asynchronous I/O; instead of waiting synchronously for an I/O operation to complete, the &lt;span class=&quot;caps&quot;&gt;CPU&lt;/span&gt; switches over to a second buffer and continues processing data. This comes at the cost of doubling memory usage (two buffers instead of one), but is probably especially beneficial for the write buffer since it&amp;#8217;s so&amp;nbsp;active.&lt;/p&gt;
&lt;p&gt;There are a few more minor performance tweaks I can think of, but no more really fundamental ones. Let me know in the comments if there&amp;#8217;s something I&amp;#8217;ve&amp;nbsp;missed.&lt;/p&gt;
&lt;p&gt;A natural extension to this is parallel sorting with multiple machines, but I don&amp;#8217;t plan on taking this little C codebase that far. Better to do it properly with Hadoop in a lot less&amp;nbsp;code.&lt;/p&gt;
&lt;h3&gt;Conclusion&lt;/h3&gt;
&lt;p&gt;My best case sorts &lt;span class=&quot;caps&quot;&gt;1GB&lt;/span&gt; of 32-bit integers in 127 seconds in &lt;span class=&quot;caps&quot;&gt;64MB&lt;/span&gt; of memory on my laptop, and I think there&amp;#8217;s at least a 2x improvement left with bigger memory sizes. I really enjoy this kind of performance analysis and tuning, since it requires thinking about the storage hierarchy, memory management, and parallelism. It&amp;#8217;s been a reasonable two-day project, and I could see this being assigned as an undergrad course project. It doesn&amp;#8217;t feel altogether too different from research either, just at a much smaller&amp;nbsp;scale.&lt;/p&gt;
&lt;p&gt;Once again, all the code is available &lt;a href=&quot;https://github.com/umbrant/extsort&quot;&gt;at github&lt;/a&gt;.&lt;/p&gt;

   </content></entry><entry><title>Album first impressions pt. 1</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/album_first_impressions_pt_1.html"/><updated>2011-04-08T00:00:00Z</updated><published>2011-04-08T00:00:00Z</published><id>http://www.umbrant.com/blog/2011/album_first_impressions_pt_1.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;I&amp;#8217;ve gotten some large influxes of music recently, posting my first impressions of eight albums I&amp;#8217;ve given a listen or two. It&amp;#8217;s a pretty eclectic mix, new and old stuff. Reviews are unordered, and only qualitative ratings. Unexpectedly, Kanye&amp;#8217;s newest work is my favorite album of 2010. More to&amp;nbsp;come.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;p&gt;&lt;strong&gt;James Brown - Love Power Peace (Live at the Olympia, Paris, 1971)&lt;/strong&gt;: I have a soft spot for live albums, and this has to be among the best. I don&amp;#8217;t have much exposure to funk (the closest thing being jazz fusion), but it&amp;#8217;s easy to fall in love with the layered big-band instrumentation, high energy, and evocative call-and-response&amp;nbsp;segments.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;&lt;span class=&quot;caps&quot;&gt;LCD&lt;/span&gt; Soundsystem - &lt;span class=&quot;caps&quot;&gt;LCD&lt;/span&gt; Soundsystem&lt;/strong&gt; and &lt;strong&gt;&lt;span class=&quot;caps&quot;&gt;LCD&lt;/span&gt; Soundsystem - Sound of Silver&lt;/strong&gt;: I liked their most recent album &lt;strong&gt;This Is Happening&lt;/strong&gt; quite a bit, so I checked out the rest of their discography. &lt;span class=&quot;caps&quot;&gt;LCD&lt;/span&gt; Soundsystem has a pretty unique sound, rough and unvarnished singing/spoken word over some of the dirtiest bass lines and drops I&amp;#8217;ve ever heard. It&amp;#8217;s easy to see why their work is such an appealing target for remixers and DJs. I&amp;#8217;d argue that James Murphy has taken the art of repetition to perfection, surpassing even French house demigods Daft Punk. I like these two albums less than This Is Happening, but they&amp;#8217;re still going on my coding/work playlist since they&amp;#8217;re quite listenable for long periods of&amp;nbsp;time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Radiohead - King of Limbs&lt;/strong&gt;: Disappointing. I had to come around to &lt;strong&gt;In Rainbows&lt;/strong&gt;, which is now my favorite Radiohead album, but I don&amp;#8217;t think I&amp;#8217;m going to acquire the taste of King of Limbs any time soon. It feels heavily influenced by dubstep, a genre of electronic music I&amp;#8217;m not particularly infatuated with. I&amp;#8217;m willing to give King of Limbs a few more listens because of brand loyalty, but it probably won&amp;#8217;t make my list of top 5 favorite Radiohead&amp;nbsp;albums.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Regina Spektor - Begin to Hope&lt;/strong&gt;: She reminds me of Feist, but with better clarity and range. I&amp;#8217;m slowly adding female vocal music to my collection (see also: Cat Power), and this is a nice find. Where Feist has this airy, carefree feeling that permeates her music, Spektor is more raw, emotional, and pure. She also has some adorable speech impediment that makes &lt;a href=&quot;http://www.youtube.com/watch?v=5zA4oG4FJFY&quot;&gt;&amp;#8220;better&amp;#8221; sound like &amp;#8220;betto&amp;#8221;&lt;/a&gt;.&amp;nbsp;Recommended.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Little Boots - Hands&lt;/strong&gt;: I think she has mild internet fame for her videos &lt;a href=&quot;http://www.youtube.com/watch?v=-CWBgm_-Ggs&quot;&gt;playing&lt;/a&gt; a &lt;a href=&quot;http://www.youtube.com/watch?v=N6tLRCDqJ2c&quot;&gt;Tenori-on&lt;/a&gt;, and now she has an album. It&amp;#8217;s pop, which is not normally my thing, so it&amp;#8217;s a bit hard for me to judge. It&amp;#8217;s a little formulaic (expected), and reminds me of Lady Gaga, with a hip hop or alt rock feel. The autotune still grates on my ears, and is a waste of her talent. Probably won&amp;#8217;t go on my regular listening rotation, but recommended for the&amp;nbsp;genre.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Robyn - Body Talk&lt;/strong&gt;: Robyn is a Swedish pop/dance artist who turned down the recording contract that eventually went to Britney Spears. Happily, Robyn also makes much better music. I&amp;#8217;ve been watching her since happening across &lt;strong&gt;Body Talk Pt &lt;span class=&quot;caps&quot;&gt;II&lt;/span&gt;&lt;/strong&gt;; 2010 saw the release of &lt;strong&gt;Body Talk Pt I&lt;/strong&gt; and &lt;strong&gt;Body Talk Pt &lt;span class=&quot;caps&quot;&gt;II&lt;/span&gt;&lt;/strong&gt; as well as this most recent album, which pulls the hits from the first two Body Talk albums and adds a few new songs, forming what is effectively a &amp;#8220;best of&amp;#8221; compilation. Releasing three albums of new material in a year is impressive; even more impressive is the quality of each song on Body Talk. I could see any song off this album being a single. Recommended even if you don&amp;#8217;t like pop. Listen to &lt;a href=&quot;http://www.youtube.com/watch?v=fA3j3VTAsTk&quot;&gt;Fembot&lt;/a&gt; and &lt;a href=&quot;http://www.youtube.com/watch?v=ketX6HITIDU&quot;&gt;Indestructible&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Kanye West - My Beautiful Dark Twisted Fantasy&lt;/strong&gt;: There&amp;#8217;s a reason this album made numerous &amp;#8220;best of 2010&amp;#8221; lists. &lt;em&gt;It&amp;#8217;s the best fucking album of 2010&lt;/em&gt;. I can hardly believe I&amp;#8217;m saying this, but I actually like it more than Arcade Fire&amp;#8217;s 2010 entry, The Suburbs. I heard &lt;span class=&quot;caps&quot;&gt;MBDTF&lt;/span&gt;&amp;#8217;s &lt;a href=&quot;http://www.youtube.com/watch?v=S-qKboHKPEA&quot;&gt;opener&lt;/a&gt;, and was already overwhelmed. I could go through and recommend each track on the album for a different reason, but I&amp;#8217;ll promote in particular &lt;a href=&quot;http://www.youtube.com/watch?v=Bm5iA4Zupek&quot;&gt;Runaway&lt;/a&gt; and &lt;a href=&quot;http://www.youtube.com/watch?v=2VOVStL0NW0&quot;&gt;Blame Game&lt;/a&gt;. Kanye was always one of the best hip-hop producers in the business, and now he&amp;#8217;s got the rapping chops to match. Sick beats abound. I love the super smooth production, every note is polished to a glossy sheen. I&amp;#8217;m impressed with the variety in styles. To conclude: I don&amp;#8217;t particularly like hip hop, I don&amp;#8217;t like Kanye&amp;#8217;s earlier work, I don&amp;#8217;t like Kanye the person, but &lt;strong&gt;My Beautiful Dark Twisted Fantasy&lt;/strong&gt; is undeniably brilliant. It&amp;#8217;s a hip hop album for the&amp;nbsp;ages.&lt;/p&gt;

   </content></entry><entry><title>Static website hosting on Amazon S3</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/static_hosting_on_s3.html"/><updated>2011-04-01T02:49:00Z</updated><published>2011-04-01T02:49:00Z</published><id>http://www.umbrant.com/blog/2011/static_hosting_on_s3.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;&lt;a href=&quot;http://en.wikipedia.org/wiki/Werner_Vogels&quot;&gt;Werner Vogels&lt;/a&gt;, Amazon &lt;span class=&quot;caps&quot;&gt;CTO&lt;/span&gt;, posted on his &lt;a href=&quot;http://www.allthingsdistributed.com/&quot;&gt;blog&lt;/a&gt; about a month ago on &amp;#8220;&lt;a href=&quot;http://www.allthingsdistributed.com/2011/02/website_amazon_s3.html&quot;&gt;New &lt;span class=&quot;caps&quot;&gt;AWS&lt;/span&gt; feature: Run your website from Amazon S3&lt;/a&gt;&amp;#8220;. S3 now offers the ability to host static &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; pages directly from an S3 bucket, which is a great alternative for small blogs and sites (provided, of course, that you don&amp;#8217;t actually need any dynamic content). This has the potential to greatly reduce your hosting costs. A small Dreamhost/Slicehost/Linode costs around $20 a month, and I used to run this site out of an extreme budget &lt;span class=&quot;caps&quot;&gt;VPS&lt;/span&gt; (&lt;a href=&quot;http://virpus.com/&quot;&gt;Virpus&lt;/a&gt;) which was only $5 a month, but I expect to be paying only a few cents per month for S3 (current pricing is just &lt;a href=&quot;http://aws.amazon.com/s3/#pricing&quot;&gt;15&amp;cent; per &lt;span class=&quot;caps&quot;&gt;GB&lt;/span&gt;-month&lt;/a&gt;). Of course, you also gain best-of-class durability, fault-tolerance, and scalability from hosting out of S3, meaning that your little site should easily survive a&amp;nbsp;slashdotting.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;

&lt;p&gt;The difficulty here is that most of the popular blogging engines require a backing database, and do their content generation dynamically server side. That doesn&amp;#8217;t fly with S3; since it is, after all, just a Simple Storage Service, content has to be static and pregenerated. I chose to use &lt;a href=&quot;http://ringce.com/hyde&quot;&gt;Hyde&lt;/a&gt;, a Python content generator that turns templates (based on the &lt;a href=&quot;http://www.djangoproject.com/&quot;&gt;Django&lt;/a&gt; templating engine) into &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt;. Hyde page templates are dynamic, written in &lt;a href=&quot;http://docs.djangoproject.com/en/dev/topics/templates/&quot;&gt;Django&amp;#8217;s templating language&lt;/a&gt; which supports variables, control flow, and hierarchal inheritance. Hyde will parse these templates, fill in the dynamic content, and finally generate static &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; pages suitable for uploading to S3. Ruby folks can check out &lt;a href=&quot;http://jekyllrb.com/&quot;&gt;Jekyll&lt;/a&gt; as an&amp;nbsp;alternative.&lt;/p&gt;
&lt;h3&gt;Caveats&lt;/h3&gt;
&lt;p&gt;To be clear, purely static content won&amp;#8217;t suffice for many sites out there (like anything with user-generated content). Even a simple blog like is only feasible because there are web services that fill in the gaps in functionality. &lt;a href=&quot;http://disqus.com/&quot;&gt;Disqus&lt;/a&gt; seems to have cornered the market for comments as a service; you just include a little bit of Javascript and it&amp;#8217;s good to go. It&amp;#8217;s similarly easy to include a &lt;a href=&quot;http://tweet.seaofclouds.com/&quot;&gt;Twitter widget showing your recent tweets&lt;/a&gt; with another little blob of Javascript, and &lt;a href=&quot;http://www.feedburner.com&quot;&gt;Feedburner&lt;/a&gt; and &lt;a href=&quot;http://www.google.com/analytics/&quot;&gt;Google Analytics&lt;/a&gt; are defacto analytics tools. There&amp;#8217;s barely a need these days to scrape, store, and serve content yourself these days, further obviating the need for a real&amp;nbsp;server.&lt;/p&gt;
&lt;p&gt;This is also clearly a more coding heavy approach to blogging and site generation than most people need. With free blog services like &lt;a href=&quot;http://wordpress.com&quot;&gt;Wordpress.com&lt;/a&gt;, &lt;a href=&quot;http://www.blogger.com&quot;&gt;Blogger&lt;/a&gt;, &lt;a href=&quot;http://www.tumblr.com&quot;&gt;Tumblr&lt;/a&gt;, and &lt;a href=&quot;http://www.posterous.com&quot;&gt;Posterous&lt;/a&gt;, blogging has never been easier or more available. &lt;a href=&quot;http://sites.google.com&quot;&gt;Google Sites&lt;/a&gt; is also a great way of throwing up a quick website. I went with S3 and Hyde because I wanted more customization in the look and feel of the site, I like the Django templating system, and I wanted to play with S3 (especially since Amazon offers &lt;a href=&quot;http://aws.amazon.com/free/&quot;&gt;1 year of free &lt;span class=&quot;caps&quot;&gt;AWS&lt;/span&gt; credit&lt;/a&gt;). I also feel a bit safer about my data, since it&amp;#8217;s backed by &lt;a href=&quot;http://aws.amazon.com/s3/faqs/#How_is_Amazon_S3_designed_to_achieve_99.999999999%_durability&quot;&gt;Amazon&amp;#8217;s eleven 9&amp;#8217;s of durability&lt;/a&gt; on S3, it&amp;#8217;s on my local machine, and &lt;a href=&quot;https://github.com/umbrant/umbrant-blog&quot;&gt;under version control at github&lt;/a&gt;.&lt;/p&gt;
&lt;h3&gt;Hyde&lt;/h3&gt;
&lt;p&gt;Hyde is pretty straightforward for anyone with experience writing Django templates, since it&amp;#8217;s basically the Django template engine plus some extra magic content and context tags. The &lt;a href=&quot;https://github.com/lakshmivyas/hyde/blob/master/README.markdown&quot;&gt;Hyde &lt;span class=&quot;caps&quot;&gt;README&lt;/span&gt;&lt;/a&gt; and &lt;a href=&quot;https://github.com/lakshmivyas/hyde/wiki&quot;&gt;github wiki&lt;/a&gt; are somewhat helpful in laying this out. Essentially, Hyde lets you assign per-page metadata that can be accessed by other pages as they walk the directory structure of your content; your &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt; structure mirrors your folder structure. By default, this metadata includes a &lt;code&gt;created&lt;/code&gt; field that fuels the magic &lt;code&gt;recents&lt;/code&gt; template tag which gets the most recent content from a directory (like your blog). There are a few more Hyde specific features which you can read about on &lt;a href=&quot;https://github.com/lakshmivyas/hyde/wiki/Templating-Guide&quot;&gt;the wiki page on templating&lt;/a&gt;, and the &lt;a href=&quot;http://docs.djangoproject.com/en/dev/ref/templates/builtins/&quot;&gt;Django templating reference&lt;/a&gt; is also&amp;nbsp;useful.&lt;/p&gt;
&lt;p&gt;I still found myself a little stuck, and what was most useful was reading the source for the skeleton site that Hyde generates for you initially, and the &lt;a href=&quot;https://github.com/sjl/stevelosh/&quot;&gt;code that Steve Losh&lt;/a&gt; uses to generate his own blog. To help you out, I&amp;#8217;ve &lt;a href=&quot;https://github.com/umbrant/umbrant-blog&quot;&gt;published the code&lt;/a&gt; for this site on github too. It might be useful to read &lt;a href=&quot;http://stevelosh.com/blog/2010/01/moving-from-django-to-hyde/&quot;&gt;Steve&amp;#8217;s write up on moving from Django to Hyde&lt;/a&gt; as&amp;nbsp;well.&lt;/p&gt;
&lt;p&gt;A few nice features of Hyde I like are the ability to automatically compress Javascript and &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt; with jsmin and cssmin, and support for writing posts in &lt;a href=&quot;http://daringfireball.net/projects/markdown/&quot;&gt;Markdown&lt;/a&gt;, which is a lot easier and more portable than &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt;. There&amp;#8217;s also support for writing &amp;#8220;higher level &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt;&amp;#8221; (CleverCSS, &lt;span class=&quot;caps&quot;&gt;HSS&lt;/span&gt;, LessCSS), but I never understood the point of these and didn&amp;#8217;t use&amp;nbsp;them.&lt;/p&gt;
&lt;p&gt;The features I had to add to the skeleton code are a draft status for posts, and the &amp;#8220;Recent Posts&amp;#8221; and &amp;#8220;Archive&amp;#8221; sections on the sidebar. Drafts were done by adding a metadata &lt;code&gt;draft: True&lt;/code&gt; tag to draft posts, and modifying all my &amp;#8220;listing&amp;#8221; pages to exclude these posts (like the home page, archive, recent posts, atom feed). The &amp;#8220;Recent Posts&amp;#8221; and &amp;#8220;Archive&amp;#8221; sidebar use &lt;code&gt;page.walk&lt;/code&gt; to traverse the blog directory and the &lt;code&gt;recents&lt;/code&gt; tag the most recent posts. These posts are then filtered with if statements to exclude non-draft content. This is all slightly hacky, since if you want to show the 5 most recent blog posts (as returned by &lt;code&gt;recents 5&lt;/code&gt;), you might have less than 5 posts after filtering out drafts. This is worked around by not dating drafts until publication (which gives them a default date in&amp;nbsp;1900).&lt;/p&gt;
&lt;p&gt;I also had to modify Hyde&amp;#8217;s &lt;code&gt;page.walk&lt;/code&gt; and &lt;code&gt;page.walk_reverse&lt;/code&gt; to walk directories in lexicographically sorted order, but I&amp;#8217;m hoping that&amp;#8217;s been fixed in git (I was using version&amp;nbsp;0.4).&lt;/p&gt;
&lt;h3&gt;S3&lt;/h3&gt;
&lt;p&gt;There is &lt;a href=&quot;http://aws.typepad.com/aws/2011/02/host-your-static-website-on-amazon-s3.html&quot;&gt;plenty&lt;/a&gt; of &lt;a href=&quot;http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?WebsiteHosting.html&quot;&gt;documentation&lt;/a&gt; on &lt;a href=&quot;http://docs.amazonwebservices.com/AmazonS3/latest/dev/index.html?WebsiteHosting.html&quot;&gt;how&lt;/a&gt; to set up an S3 bucket as a website. It&amp;#8217;s pretty easy, I didn&amp;#8217;t have any trouble with&amp;nbsp;this.&lt;/p&gt;
&lt;p&gt;Making your existing domain name point to your S3 bucket is a little more tricky. S3 provides a &lt;span class=&quot;caps&quot;&gt;URL&lt;/span&gt; for your bucket (in my case, &lt;a href=&quot;http://www.umbrant.com.s3-website-us-west-1.amazonaws.com/&quot;&gt;http://www.umbrant.com.s3-website-us-west-1.amazonaws.com/&lt;/a&gt;). The first problem is a limitation of &lt;span class=&quot;caps&quot;&gt;DNS&lt;/span&gt;: you can&amp;#8217;t make your zone apex a &lt;span class=&quot;caps&quot;&gt;CNAME&lt;/span&gt;. If that was gibberish, it means that you can&amp;#8217;t make your plain domain name (&lt;a href=&quot;http://umbrant.com&quot;&gt;http://umbrant.com&lt;/a&gt;) an alias for another domain name, like your S3 bucket&amp;#8217;s. Subdomains don&amp;#8217;t have this limitation, which is why you&amp;#8217;re viewing this blog at &lt;a href=&quot;http://www.umbrant.com&quot;&gt;http://www.umbrant.com&lt;/a&gt;, happily &lt;span class=&quot;caps&quot;&gt;CNAME&lt;/span&gt;&amp;#8217;d to my S3 bucket. My zone apex then does a redirect to the &lt;code&gt;www&lt;/code&gt; subdomain; this redirect is a service provided by some registrars, or you can beg a friend with a&amp;nbsp;server.&lt;/p&gt;
&lt;p&gt;I just lied to you a little about how this works. Notice that if you &lt;code&gt;dig www.umbrant.com&lt;/code&gt;, you get the&amp;nbsp;following:&lt;/p&gt;
&lt;div class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;nv&quot;&gt;$&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;dig&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;www&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;umbrant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;sr&quot;&gt;&amp;lt;snip&amp;gt;&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;QUESTION&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;SECTION&lt;/span&gt;:&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;www&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;umbrant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;     &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;IN&lt;/span&gt;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;p&quot;&gt;;;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;ANSWER&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;SECTION&lt;/span&gt;:&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;www&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;umbrant&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;   &lt;span class=&quot;mi&quot;&gt;831&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;IN&lt;/span&gt;&lt;/span&gt;   &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;CNAME&lt;/span&gt;&lt;/span&gt;  &lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;website&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;west&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amazonaws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;s3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;website&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;us&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;west&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;amazonaws&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;com&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;60&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;IN&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;A&lt;/span&gt;  &lt;span class=&quot;mf&quot;&gt;204.246.162.151&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;sr&quot;&gt;&amp;lt;snip&amp;gt;&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;

&lt;p&gt;My subdomain isn&amp;#8217;t actually &lt;span class=&quot;caps&quot;&gt;CNAME&lt;/span&gt;&amp;#8217;d to my S3 bucket domain name, I&amp;#8217;ve set it to alias directly to &lt;code&gt;s3-website-us-west-1.amazonaws.com&lt;/code&gt;. This is a mild optimization that saves a &lt;span class=&quot;caps&quot;&gt;DNS&lt;/span&gt; lookup; if you &lt;code&gt;dig&lt;/code&gt; my bucket domain name, you see that it&amp;#8217;s &lt;span class=&quot;caps&quot;&gt;CNAME&lt;/span&gt;&amp;#8217;d to &lt;code&gt;s3-website-us-west-1.amazonaws.com&lt;/code&gt; anyway, which finally gets turned into the &lt;span class=&quot;caps&quot;&gt;IP&lt;/span&gt; address for an S3 server (the A record). This server uses the referring domain name (&lt;code&gt;www.umbrant.com&lt;/code&gt;) to look up the S3 bucket with the same name. This system also means that if someone&amp;#8217;s already made a bucket in your region with the same name as your subdomain, you&amp;#8217;ve got to choose a different subdomain (thanks to S3&amp;#8217;s flat keyspace). In other words, when using S3, your bucket name and subdomain must be the&amp;nbsp;same.&lt;/p&gt;
&lt;p&gt;Uploading files to S3 isn&amp;#8217;t too bad. I&amp;#8217;m sure there are existing tools out there for interfacing with S3 on the commandline, but I rolled my own in Python with the &lt;a href=&quot;http://pypi.python.org/pypi/simples3/1.0&quot;&gt;SimpleS3&lt;/a&gt; library available on PyPI. It&amp;#8217;s basically rsync-for-S3 with some issues; it doesn&amp;#8217;t delete old files from S3, the parsing isn&amp;#8217;t bulletproof, and it uses modtimes to check for updates instead of checksums (which I plan on implementing soon, right now it&amp;#8217;s almost my entire blog each time I re-run Hyde). However, it does work, and it is really simple to&amp;nbsp;use.&lt;/p&gt;
&lt;div class=&quot;code&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;simples3&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;*&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;os&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;re&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# Config options&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;ACCESS_KEY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;YOUR_ACCESS_KEY&amp;#39;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;SECRET_KEY&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;YOUR_SECRET_KEY&amp;#39;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# Change this&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;BUCKET_NAME&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;  &lt;span class=&quot;s&quot;&gt;&amp;quot;www.umbrant.com&amp;quot;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# Change this too, make sure to edit your region and bucket name&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;BASE_URL&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;#39;https://s3-us-west-1.amazonaws.com/www.umbrant.com&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# &lt;span class=&quot;caps&quot;&gt;NO&lt;/span&gt; &lt;span class=&quot;caps&quot;&gt;TRAILING&lt;/span&gt; &lt;span class=&quot;caps&quot;&gt;SLASH&lt;/span&gt;&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;SOURCE_DIR&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/home/andrew/dev/umbrant_static/deploy&amp;quot;&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;IGNORE&lt;/span&gt;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;br /&gt;          &lt;span class=&quot;s&quot;&gt;&amp;quot;\.(.*).swp$&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;~$&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;# ignore .swp files&lt;/span&gt;&lt;br /&gt;         &lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# code&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;ignore_re&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[]&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;&lt;span class=&quot;caps&quot;&gt;IGNORE&lt;/span&gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;    &lt;span class=&quot;n&quot;&gt;ignore_re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;append&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;compile&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;))&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# open bucket&lt;/span&gt;&lt;br /&gt;&lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;S3Bucket&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BUCKET_NAME&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;access_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;ACCESS_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;br /&gt;                  &lt;span class=&quot;n&quot;&gt;secret_key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SECRET_KEY&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;base_url&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;BASE_URL&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;c&quot;&gt;# recursively put in all files in SOURCE_DIR&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dirs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;walk&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOURCE_DIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;br /&gt;    &lt;span class=&quot;n&quot;&gt;relroot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;len&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;SOURCE_DIR&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;+&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:]&lt;/span&gt;&lt;br /&gt;    &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;files&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;c&quot;&gt;# root directory files should not have a preceding &amp;quot;/&amp;quot;&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;c&quot;&gt;# puts the files in a blank named directory, not what we want&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relroot&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;relroot&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;else&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;root&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;/&amp;quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;+&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;        &lt;span class=&quot;c&quot;&gt;# check in the ignore list&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;ignore&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;i&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;in&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ignore_re&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;re&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;match&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;f&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;br /&gt;                &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Ignoring&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;br /&gt;                &lt;span class=&quot;n&quot;&gt;ignore&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;True&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;ignore&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;os&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;modtime&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st_mtime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)}&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;        &lt;span class=&quot;c&quot;&gt;# check if it&amp;#39;s changed with modtimes&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;try&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;info&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;except&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;contents&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;public-read&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Uploading&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;not&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;metadata&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;modtime&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;ow&quot;&gt;or&lt;/span&gt; \&lt;br /&gt;        &lt;span class=&quot;n&quot;&gt;sf&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;metadata&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;][&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;modtime&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;str&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;stat&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;st_mtime&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;n&quot;&gt;bucket&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;put&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;filename&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;read&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;acl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&amp;quot;public-read&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;br /&gt;                       &lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Uploading&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;br /&gt;            &lt;span class=&quot;k&quot;&gt;continue&lt;/span&gt;&lt;br /&gt;&amp;nbsp;&lt;br /&gt;        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&amp;quot;Skipping&amp;quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;key&lt;/span&gt;&lt;br /&gt;&lt;/pre&gt;&lt;/div&gt;&lt;br /&gt;&lt;/div&gt;

&lt;h3&gt;Final&amp;nbsp;remarks&lt;/h3&gt;
&lt;p&gt;This was a pretty reasonable and fun 2 days of effort, most of which was spent on tuning the &lt;span class=&quot;caps&quot;&gt;CSS&lt;/span&gt; template and writing content, not wrangling code. Hyde doesn&amp;#8217;t feel very mature (documentation is lacking, example skeleton site is slightly broken, the sorting bug), but it works well enough and is good for people transitioning from Django. I&amp;#8217;m very positive about S3 and Amazon Web Services in general (&lt;a href=&quot;http://blog.reddit.com/2011/03/why-reddit-was-down-for-6-of-last-24.html&quot;&gt;modulo Elastic Block Store being terrible&lt;/a&gt;, but that&amp;#8217;s a rant for another day), since my site is now essentially impervious to failure. It&amp;#8217;s also pleasing to see top management like Werner Vogels dogfooding Amazon&amp;#8217;s&amp;nbsp;features.&lt;/p&gt;

   </content></entry><entry><title>Hello world!</title><author><name>Andrew Wang</name></author><link href="http://www.umbrant.com/blog/2011/hello_world.html"/><updated>2011-03-30T01:34:29Z</updated><published>2011-03-30T01:34:29Z</published><id>http://www.umbrant.com/blog/2011/hello_world.html</id><content type="html">
       

&lt;!-- Hyde::Excerpt::Begin --&gt;

&lt;p&gt;Hello world! This is my new blog and profile site, purely static &lt;span class=&quot;caps&quot;&gt;HTML&lt;/span&gt; hosted out of S3. 
Since I find this to be a neat trick, I&amp;#8217;m going to make the howto into my second-ever blog post.
This site is still very much under development, I have lots of ideas for neat features I want to add (comments being the first), but it&amp;#8217;s good enough to put&amp;nbsp;live.&lt;/p&gt;
&lt;!-- Hyde::Excerpt::End --&gt;


   </content></entry></feed>
