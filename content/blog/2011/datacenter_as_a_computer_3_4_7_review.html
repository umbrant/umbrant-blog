{% extends "_post.html" %}

{%load webdesign %}

{%hyde
    title: "The Datacenter as a Computer Ch. 3, 4, 7 review"
    created: 2011-09-07 15:19:00
    draft: True
%}

{% block article %}

{%article%}

{% excerpt %}

This is a review of chapters 3, 4, and 7 from "The Datacenter as a Computer". The topics covered are hardware for servers, datacenter basics, and fault-tolerance and recovery. 

Since this is a broad set of topics, the discussion here will be a bit disjointed.

Main idea
Problems, are they real?
fundamental tradeoffs
long-term impact

{%endexcerpt%}

## Main idea

The three chapters here cover essentially how to design the hardware, software, and operational concerns of a datacenter. 

In terms of datacenter hardware, the main concern is choosing the most cost-efficient type of node that still runs your workload sufficiently fast.

Operationally, the concerns deal with power and cooling of nodes, factors which limit the density of nodes in a datacenter. Cooling can account for a major part of the power cost of a datacenter, and AC is just as critical a service as power since the datacenter can survive for only a matter of minutes if the AC unit dies.

The chapter on fault-tolerance and recovery talks about the different types of faults that can present in hardware and software, and how they might affect service availability. The ultimate goal of the service is to be able to survive faults without significantly affecting availability, either through overprovisioning or graceful degradation of service quality.

## Problems presented

#### Hardware/software scaling
*   How low can you go? Small nodes are more cost-efficient than beefy ones in terms of computation-per-watt and price-per-computation, but might start becoming a bottleneck due to the limits of request parallelism. Further parallelization of an application can be really painful, and having to deal with coordination between more nodes has its drawbacks. Never forget Amdahl's Law: if the serial parts of your program dominate, and you're running that serial code on slow nodes, your program is going to run slowly too. Beefy nodes can run the serial part quickly.
*   Another point about small nodes is that they are harder to schedule efficiently. Resources effectively get fragmented; there might not be enough left on a small node to schedule a new task. Big nodes pack more efficiently.

#### Operations

#### Fault-tolerance

## Tradeoffs

*   Programming cost vs. hardware cost. Can speed up either by throwing programmers at a problem (squeeze more parallelism from the code), or by buying better hardware (run the same code faster). This is an economic balancing act.
*   

## Impact

Amdahl's law and the limits of parallelism
low end vs. high end server tradeoffs
*   request level parallelism is slower on low end
*   low end is more power efficient, but harder to run at high load
*   low end is more cost effective
*   low end can be better when I/O bound, wider bus
software/hardware codesign
fungible resources are the best

cooling is a major concern
hot and cold aisles, pressurized cold air in the floor goes through perforated floor tiles
UPS are generally not actually on the datacenter floor, they're instead external and the power distribution unit (PDU) is near the racks
Coolant is cooled back down via a chiller tower, or "free cooling" using the external environment
cooling costs can be really expensive: 40% of load
in-rack cooling is more effective at increasing power density, but has concerns about spillage of coolant onto the datacenter floor
container-based datacenters are also really effective at increasing power density

only need about 4 9s of fault tolerance, since you're also subject to internet outages.
fault tolerance is a nice property to have, since it masks failures and makes upgrades easier
most faults are caused by software errors, not hardware. machines last 6 years between faults on average.
being fault-tolerant makes repairs less urgent and uses technicians more efficiently

{%endarticle%}

{% endblock %}
