{% extends "_post.html" %}

{%load webdesign %}

{%hyde
    title: Windows Azure Storage
    created: 2016-02-03 21:23:00
    draft: True
%}

{% block article %}

{%article%}

{% excerpt %}

What makes this paper special is that (I believe) it is the only published paper about a production *cloud* blobstore. The 500-pound gorilla in this space is Amazon S3, but I find (WAS) the more interesting system since it provides strong consistency, additional features like append, and serves as the backend for not just WAS Blobs, but also WAS Tables (structured data access) and WAS Queues (message delivery). It also occupies a different design point than hash-partitioned blobstores like Swift and Rados.

This paper, "Windows Azure Storage: A Highly Available Cloud Storage Service with Strong Consistency" by Calder et al., was published at SOSP '11.

{%endexcerpt%}

### Background

Most people are familiar with filesystems. Filesystems have a hierarchical namespace composed of a nested directory tree, where directories contain files. Directories and files can have various bits of metadata attached to them like permissions, modtime, ACLs, xattrs, etc. Directories are used to logically group files together, and there are commands that work on entire recursive directory trees (mv, rm -r, chown -r, etc).

Blobstores are like filesystems, except simpler. A unifying characteristic of blobstores is that they do not provide a hierarchical namespace. Instead, you get multiple flat namespaces (which S3 calls *buckets* and WASB *partitions*), in which you can store blobs. Blobstores also provide fewer features than filesystems. It's typical to not support an operation like rename, setting per-blob permissions, and also have preconditions around IO (e.g. S3 requires a full-blob checksum at upload time, and no random writes or appends) which push complexity to the application-level.

You might read this and think that blobstores sound terrible, but there's a very good reason for throwing away these features: horizontal scalability. It's difficult to shard a hierarchical namespace, and it's even harder to support operations like directory rename, so blobstores punt on these problems. As a result, you have a system that architecturally has infinite scale.

### Overview

In the datacenter, WAS is composed of *stamps*, which are sets of 10-20 racks of servers. This is what others might call a *cell* or *pod*, it's used as a unit of deployment and management. There are many stamps per datacenter, and many datacenters which are geographically distributed for fault-tolerance.

Users have *accounts* and all of the data in an account is stored in a stamp in a chosen geographic region. Accounts are another unit of management, and are migrated between stamps based on load.

WAS is a very layered system, so let's take it from the bottom up, starting with how it works within a single stamp, then talking about how multiple stamps are glued together into a global namespace.

### Stream Layer

The bottom-most layer in the stack is the *stream layer*. The stream layer lets you interact with a flat namespace of append-only logs called *streams*. Streams are composed of *extents*, which are a unit of replication and are about *1GB* and stored on as files on a local filesystem. Only the last extent in the stream can be appended to. Extents in turn are composed of *blocks*, which variable-length up to 4MB in size, and are the unit of a client read or write. Blocks are also the unit of checksumming, so the entire block is read at read time to verify the checksum.

Architecturally, this looks a lot like HDFS. There is a Paxos-replicated *stream master* which maintains a mapping of streams-to-extents and extents-to-nodes. It chooses which nodes to use for incoming writes, routes reads to the correct and re-replicates extents when nodes fail. The stream master needs to keep these mappings in memory, and it's designed for approximately 100k streams and 50 million total extents.

Notably, the stream master does not track the extent-to-block mapping, which would not fit on a single machine. Instead, this is handled by the extent nodes, which maintain an index of the block offsets alongside the extent file.

The stream layer uses chain replication when writing an extent, with some nice changes compared to HDFS. The first simplification is that the three extent nodes are used for writes is fixed while the extent is unsealed (meaning accepting writes), and one of the three is the primary which replicates to the two secondaries. Writes to extents are at the granularity of entire blocks, and these block writes are atomic (and there is even an atomic multi-block append). Combined, this has the nice property of allowing concurrent writers to an extent, since the primary orders the incoming block appends.

Failures during a write are handled by sealing the extent and starting a new one. Sealing the extent requires agreeing on the length of the extent, which is coordinated with the stream master. The SM asks the remaining nodes for the length of the extent, and uses the smallest length. This is safe since only writes are only ack'd to the client after they are fully-replicated. Longer lengths are also okay, since stream clients are required to handle duplicate blocks in a stream. Once sealed, this is the final length of the extent. If a version of the extent with a different length appears, it is safely discarded.

The stream layer also implements background erasure coding of sealed extents, as well as latency-levelling by a similar mechanism to Jeff Dean's [The Tail at Scale](http://research.google.com/pubs/pub40801.html) work. They also allocate a separate SSD for incoming writes and asynchronously flush to data drives for better latency, and do some deadline IO scheduling to further improve latency.

### Partition Layer

Now, we move onto the co-designed user of the stream layer: the *partition layer*, which maintains user-visible constructs like blobs, tables, and queues.

The partition layer is a range-partitioned distributed database. These ranges can be split and merged and moved around based on load.

There is a table for each of the three user-visible constructs, a table that describes the schema of these three tables, and finally a table of the mapping of ranges to servers (like the meta table in HBase). The primary key for the three *object tables* is a compound key of *(account, partition, object name)*, and other columns describe in what stream, extent, offset, and length have the corresponding data for that object. Since it's a range-partitioned distributed database that uses LSM-trees under the hood, I'm going to point to [Kudu](http://getkudu.io/) and [HBase](https://hbase.apache.org/) as similar systems (particularly Kudu). Things like the memstore, bloom filters, and row caching are present in all of these systems, and I imagine they share other tricks too.

Each range uses a couple streams to maintain its state. The two important ones are a commit stream (a WAL) and a row data stream (checkpoints of WAL mutations, HFiles in HBase parlance) which are used to maintain the LSM tree. They also implement a BLOB type which writes blob data into a side stream to avoid the write amplification of LSM trees, instead using pointers and efficient stream concat operations to avoid rewriting data.

One interesting point is that on a per stamp basis, they see 75 splits and merges and 200 partition moves every day. That's a lot more than I would have guessed for HBase, but since the partition layer doesn't have storage locality, moving a partition is cheap, and efficient stream concat means you can possibly avoid rewriting a lot of data when doing splits/merges.

### Inter-stamp replication

This is all managed by something called the *Location Service*, which maintains the mapping of account-to-stamp and updates DNS and VIPs accordingly.

### Questions and comments

#### How many hops / writes?

It's interesting to count how many network hops and how many writes are required to persist a blob in this system. Let's try and count:

1. 

* Since accounts are pinned to a single stamp, that means an account is limited by the performance and capacity of a stamp. This explains what I've heard about WAS users sharding their usage across multiple accounts. Apparently accounts are a lightweight entity in Azure, but it still seems annoying that the user potentially needs to manage this.

{%endarticle%}

{% endblock %}
